{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentence Tokenization for Transformer Neural Network (English → Kannada Translation)\n",
        "\n",
        "This notebook prepares the **data processing pipeline** for a Transformer model that translates English to Kannada (a South Indian language). The pipeline covers:\n",
        "\n",
        "1. **Vocabulary Creation** — Building character-level vocabularies for both languages\n",
        "2. **Dataset Loading & Filtering** — Loading sentence pairs, removing invalid/long sentences\n",
        "3. **Tokenization** — Converting characters to integer indices with special tokens (START, END, PADDING)\n",
        "4. **Batching** — Grouping sentence pairs using PyTorch DataLoader for parallel training\n",
        "5. **Masking** — Creating padding masks and look-ahead masks to control attention\n",
        "6. **Sentence Embedding** — Combining token embeddings with positional encoding\n",
        "\n",
        "**Key design choice:** We use **character-level** tokenization (not word-level) to reduce model parameters and speed up inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n",
        "\n",
        "- **torch**: PyTorch deep learning framework, used for tensor operations, building the model, and training.\n",
        "- **numpy**: Numerical computing library, used here for percentile calculations and array operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6lvIJaCEjPY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define File Paths, Special Tokens & Language Vocabularies\n",
        "\n",
        "### Dataset\n",
        "The dataset comes from a research paper covering English paired with 11 Indian languages. We use the English-Kannada (`.en` / `.kn`) pair. The full dataset is ~20 GB with ~4 million sentence pairs.\n",
        "\n",
        "### Special Tokens\n",
        "Three special tokens are essential for sequence modeling:\n",
        "- **`<START>`**: Marks the beginning of a sentence (used for Kannada/target during generation)\n",
        "- **`<END>`**: Marks the end of a sentence\n",
        "- **`<PADDING>`**: Fills shorter sentences to a fixed length so all sentences in a batch have equal size\n",
        "\n",
        "### Vocabularies\n",
        "We build **character-level** vocabularies for each language:\n",
        "- **Kannada** is an **alpha-syllabary** — each character represents a syllable (e.g., consonant 'ಕ' + vowel marker 'ಾ' = 'ಕಾ' \"ka\"). The vocabulary includes vowels, consonants, vowel markers (matras), digits, and punctuation.\n",
        "- **English** uses a standard **phonetic alphabet** — uppercase, lowercase, digits, and punctuation.\n",
        "\n",
        "Each vocabulary maps every possible character to a unique integer index for embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Sf9_Y-E564"
      },
      "outputs": [],
      "source": [
        "# --- File paths to the parallel corpus (English-Kannada sentence pairs) ---\n",
        "# These files are stored on Google Drive; each line contains one sentence.\n",
        "english_file = 'drive/MyDrive/translation_en_kn/train.en'\n",
        "kannada_file = 'drive/MyDrive/translation_en_kn/train.kn'\n",
        "\n",
        "# --- Special Tokens ---\n",
        "# START_TOKEN: Prepended to target (Kannada) sentences so the decoder has an initial input to begin generation.\n",
        "# PADDING_TOKEN: Appended to short sentences to make all sequences the same fixed length (required for batching).\n",
        "# END_TOKEN: Appended to mark where a sentence ends, so the model learns when to stop generating.\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "# --- Kannada Vocabulary (character-level) ---\n",
        "# Kannada is an alpha-syllabary script. Characters include:\n",
        "#   - Independent vowels (ಅ, ಆ, ಇ, ... ಔ)\n",
        "#   - Consonants (ಕ, ಖ, ಗ, ... ಹ) organized in groups (velar, palatal, retroflex, dental, labial, etc.)\n",
        "#   - Dependent vowel signs / matras (ಾ, ಿ, ೀ, ು, ೂ, ... ್) that attach to consonants\n",
        "#   - Kannada digits (೦-೯), punctuation, and the special tokens\n",
        "# Total vocab size ≈ 120+ characters\n",
        "kannada_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ', \n",
        "                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', \n",
        "                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ',  # Vowels\n",
        "                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ',           # Velar consonants\n",
        "                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ',           # Palatal consonants\n",
        "                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ',           # Retroflex consonants\n",
        "                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ',           # Dental consonants\n",
        "                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ',           # Labial consonants\n",
        "                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ',  # Other consonants\n",
        "                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ',  # Vowel signs (matras) & modifiers\n",
        "                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯',  # Kannada digits 0-9\n",
        "                      PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "# --- English Vocabulary (character-level) ---\n",
        "# Standard ASCII characters: uppercase A-Z, lowercase a-z, digits 0-9, punctuation, and special tokens.\n",
        "# Total vocab size ≈ 100 characters\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@', \n",
        "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', \n",
        "                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', \n",
        "                        'Y', 'Z',\n",
        "                        '[', '\\\\', ']', '^', '_', '`', \n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
        "                        'y', 'z', \n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding Kannada Script Structure\n",
        "\n",
        "Kannada is an **alpha-syllabary** (abugida), meaning each character represents a full syllable rather than a single phoneme like in English.\n",
        "\n",
        "- A consonant like **ಕ** ('ka') inherently carries the vowel 'a'.\n",
        "- Adding a **vowel sign (matra)** changes the vowel sound: **ಕ** + **ಾ** = **ಕಾ** ('kaa').\n",
        "- The word **ಕನ್ನಡ** (\"Kannada\") is split into individual Unicode characters by Python's `list()`.\n",
        "\n",
        "This structural difference means the character set for Kannada is much larger than English, and the way characters combine is fundamentally different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0PcdLN_lmUT",
        "outputId": "d07d14cd-a966-4f1d-e935-756f499f5163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಕ', 'ನ', '್', 'ನ', 'ಡ']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Demo: Break the Kannada word \"ಕನ್ನಡ\" (Kannada) into individual Unicode characters.\n",
        "# Python's list() splits the string into its constituent Unicode code points.\n",
        "# This shows how the model will see each character as a separate token.\n",
        "text = 'ಕನ್ನಡ'\n",
        "list(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "JuvbnQVOoHr_",
        "outputId": "e64cbda7-ee9c-47eb-ccd9-ead8d44ee5fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ಕಾ'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Demo: Combining a Kannada consonant with a vowel sign (matra).\n",
        "# 'ಕ' (ka) + 'ಾ' (aa matra) = 'ಕಾ' (kaa)\n",
        "# This illustrates how Kannada syllables are formed by combining base consonants with vowel markers.\n",
        "'ಕ' + 'ಾ'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Vocabulary Index Mappings\n",
        "\n",
        "We create **bidirectional dictionaries** for each language:\n",
        "- **`index_to_language`**: Maps integer index → character (used when decoding model output back to text)\n",
        "- **`language_to_index`**: Maps character → integer index (used when converting input text to numerical tokens)\n",
        "\n",
        "These mappings are the foundation of **character-level tokenization** — every character in a sentence will be replaced by its integer index before being fed to the model's embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1WoSOS5FH2U"
      },
      "outputs": [],
      "source": [
        "# Build index-to-character and character-to-index dictionaries for both languages.\n",
        "# enumerate() gives each character a unique integer starting from 0.\n",
        "# Example: index_to_kannada = {0: '<START>', 1: ' ', 2: '!', ...}\n",
        "# Example: kannada_to_index = {'<START>': 0, ' ': 1, '!': 2, ...}\n",
        "index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}\n",
        "kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load the Dataset\n",
        "\n",
        "The dataset contains ~4 million English-Kannada sentence pairs. For faster training, we only load the **first 100,000 pairs**.\n",
        "\n",
        "Each file has one sentence per line, so `readlines()` loads all sentences into a list. We strip trailing newline characters (`\\n`) for clean processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRetf6-9FJ8p"
      },
      "outputs": [],
      "source": [
        "# Load English and Kannada sentence files (one sentence per line)\n",
        "with open(english_file, 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "with open(kannada_file, 'r') as file:\n",
        "    kannada_sentences = file.readlines()\n",
        "\n",
        "# Limit to the first 100,000 sentence pairs for faster training\n",
        "# The full dataset has ~4 million pairs but would take much longer to train on\n",
        "TOTAL_SENTENCES = 100000\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]\n",
        "\n",
        "# Strip trailing newline characters from each sentence for clean processing\n",
        "english_sentences = [sentence.rstrip('\\n') for sentence in english_sentences]\n",
        "kannada_sentences = [sentence.rstrip('\\n') for sentence in kannada_sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preview: Sample English and Kannada Sentences\n",
        "\n",
        "Let's look at the first 10 sentences from each language to verify the data loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmrrz9ZZFRi1",
        "outputId": "99e1cbfc-9c4a-4f45-f08e-364de5d28534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hes a scientist.',\n",
              " \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\",\n",
              " '8 lakh crore have been looted.',\n",
              " 'I read a lot into this as well.',\n",
              " \"She was found dead with the phone's battery exploded close to her head the following morning.\",\n",
              " 'How did mankind come under Satans rival sovereignty?',\n",
              " 'And then I became Prime Minister.',\n",
              " 'What about corruption?',\n",
              " 'No differences',\n",
              " '\"\"\"The shooting of the film is 90 percent done.\"']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preview first 10 English sentences (source language)\n",
        "english_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9jdseUqFSEb",
        "outputId": "79d14c06-5658-4c57-b500-86a6f7625ff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.',\n",
              " 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.',\n",
              " 'ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್\\u200cನ ಬ್ಯಾಟರಿ ಸ್ಫೋಟಗೊಂಡು ಆಕೆ ಮೃತಪಟ್ಟಿದ್ದಾಳೆ ಎನ್ನಲಾಗಿದೆ.',\n",
              " 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?',\n",
              " 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.',\n",
              " 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?',\n",
              " '‘ಅನುಪಾತದಲ್ಲಿ ವ್ಯತ್ಯಾಸವಿಲ್ಲ’',\n",
              " 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preview first 10 Kannada sentences (target language)\n",
        "kannada_sentences[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyze Sentence Length Distribution\n",
        "\n",
        "Since we use **character-level** tokenization, each character becomes one token. We need to understand the distribution of sentence lengths to choose an appropriate `max_sequence_length`.\n",
        "\n",
        "- **Maximum lengths** show the longest sentences (outliers) — these are rare and can be discarded.\n",
        "- **Percentile analysis** (e.g., 97th percentile) shows the length below which 97% of sentences fall, helping us choose a practical cutoff that covers most data without wasting computation on very long sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBQmCJuqza3a",
        "outputId": "d4bb6033-e1b0-49f3-8137-ee5176f9e825"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(639, 722)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the maximum character-level sentence length in each language.\n",
        "# Kannada max ≈ 639 chars, English max ≈ 722 chars — these are outlier sentences.\n",
        "max(len(x) for x in kannada_sentences), max(len(x) for x in english_sentences),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8m1B0P3FUFX",
        "outputId": "f28b333d-bfc9-4d16-dd63-284ff7897d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length Kannada: 172.0\n",
            "97th percentile length English: 179.0\n"
          ]
        }
      ],
      "source": [
        "# Calculate the 97th percentile of sentence lengths.\n",
        "# This tells us: 97% of sentences are shorter than this length.\n",
        "# Helps us pick a max_sequence_length that covers most sentences without being too large.\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Filter Sentences by Length and Vocabulary Validity\n",
        "\n",
        "We set **`max_sequence_length = 200`** characters and discard sentences that:\n",
        "1. **Exceed the max length** — Reduces dimensionality, simplifies the model, avoids overfitting to rare long sentences.\n",
        "2. **Contain characters not in our vocabulary** — Ensures every character can be tokenized.\n",
        "\n",
        "Two helper functions are used:\n",
        "- **`is_valid_tokens()`**: Checks that every unique character in the sentence exists in the vocabulary.\n",
        "- **`is_valid_length()`**: Checks that the sentence length is less than `max_sequence_length - 1` (leaving room for the END token).\n",
        "\n",
        "After filtering, the dataset reduces from 100,000 to ~81,900 valid sentence pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVx4oG8OFaJo",
        "outputId": "ed8aabc7-f7c9-4d05-ba08-222429132f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 100000\n",
            "Number of valid sentences: 81916\n"
          ]
        }
      ],
      "source": [
        "# Maximum number of characters (tokens) per sentence, including special tokens.\n",
        "# Sentences longer than this will be discarded to reduce model complexity.\n",
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    \"\"\"Check if every unique character in the sentence exists in the vocabulary.\n",
        "    Returns False immediately if any character is not found (out-of-vocabulary).\"\"\"\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    \"\"\"Check if the sentence fits within max_sequence_length.\n",
        "    We use (max_sequence_length - 1) because we need to reserve 1 position for the END token.\"\"\"\n",
        "    return len(list(sentence)) < (max_sequence_length - 1)\n",
        "\n",
        "# Iterate over all sentence pairs and collect indices of valid ones.\n",
        "# A sentence pair is valid if:\n",
        "#   1. Both Kannada and English sentences are within the max length\n",
        "#   2. All characters in the Kannada sentence are in the Kannada vocabulary\n",
        "# Note: English vocabulary validation is skipped here (handled by the broader ASCII charset)\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(kannada_sentences)):\n",
        "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(kannada_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(kannada_sentence, kannada_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(kannada_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply the Filter\n",
        "\n",
        "Keep only the valid sentence pairs by selecting sentences at the valid indices. Both English and Kannada lists are filtered in parallel to maintain alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBzpWRdqFeaQ"
      },
      "outputs": [],
      "source": [
        "# Rebuild sentence lists using only the valid indices.\n",
        "# This ensures English and Kannada sentences remain aligned (same index = same pair).\n",
        "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiSCU6iuFgWu",
        "outputId": "81f3e9c5-16af-4fd4-c934-81d966495a1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify: preview first 3 filtered Kannada sentences\n",
        "kannada_sentences[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create a Custom PyTorch Dataset\n",
        "\n",
        "PyTorch's `Dataset` class provides a standard interface for loading data. We create a custom `TextDataset` that:\n",
        "- Stores parallel English and Kannada sentence lists\n",
        "- Returns a **(English, Kannada) tuple** for a given index via `__getitem__`\n",
        "- Reports dataset size via `__len__`\n",
        "\n",
        "This is needed because default PyTorch datasets don't handle paired multilingual text data out of the box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5OA__hHoid"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for paired English-Kannada sentences.\n",
        "    \n",
        "    Each sample is a tuple: (english_sentence_string, kannada_sentence_string).\n",
        "    The DataLoader will batch these tuples together for training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, english_sentences, kannada_sentences):\n",
        "        \"\"\"Store the parallel sentence lists.\"\"\"\n",
        "        self.english_sentences = english_sentences\n",
        "        self.kannada_sentences = kannada_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of sentence pairs.\"\"\"\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return the (English, Kannada) sentence pair at the given index.\"\"\"\n",
        "        return self.english_sentences[idx], self.kannada_sentences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chdO2iYhIn_K"
      },
      "outputs": [],
      "source": [
        "# Instantiate the dataset with our filtered sentence pairs\n",
        "dataset = TextDataset(english_sentences, kannada_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfaWiz_8Iofr",
        "outputId": "cf854361-a810-4c00-92fa-1966f7b58259"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "81916"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the total number of valid sentence pairs in the dataset (~81,900)\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dymqlSxZIqeg",
        "outputId": "9c50cdcd-a5eb-4961-af74-7d3baf51d341"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\",\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access a single sample: returns (english_sentence, kannada_sentence) tuple\n",
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Batching with DataLoader\n",
        "\n",
        "**Batching** groups multiple sentence pairs to be processed simultaneously. Benefits:\n",
        "- **Reduces parameter updates**: One batch → one gradient update (instead of one per sentence).\n",
        "- **Speeds up training**: Parallelizes computation across samples in the batch.\n",
        "- **Smoother gradients**: Averaging loss over multiple samples produces more stable gradient signals.\n",
        "\n",
        "We use `batch_size = 3` here for demonstration. In practice, larger batch sizes (e.g., 32, 64) are used.\n",
        "\n",
        "The `DataLoader` automatically handles splitting the dataset into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xQZ-bUsIukw"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader that groups sentence pairs into batches of 3.\n",
        "# Each batch will contain (list_of_3_english_sentences, list_of_3_kannada_sentences).\n",
        "batch_size = 3 \n",
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)  # Create an iterator to manually step through batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMMZECktIyip",
        "outputId": "325eca43-8d2d-44ec-8793-3748f921ba75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Hes a scientist.', \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\", '8 lakh crore have been looted.'), ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.')]\n",
            "[('I read a lot into this as well.', 'How did mankind come under Satans rival sovereignty?', 'And then I became Prime Minister.'), ('ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.', 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?', 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.')]\n",
            "[('What about corruption?', '\"\"\"The shooting of the film is 90 percent done.\"', 'the Special Statute'), ('ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?', 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.', 'ವಿಶೇಷ ಕಾನೂನು')]\n",
            "[('\"Then the king said to Ittai the Gittite, \"\"Why do you also go with us? Return, and stay with the king. for you are a foreigner, and also an exile. Return to your own place.\"', 'What happened at the UN General Assembly?', 'The meeting was attended by Prime Minister Narendra Modi, Home Minister Amit Shah and Defence Minister Rajnath Singh, among others.'), ('ಆಗ ಅರಸನು ಗಿತ್ತೀಯನಾದ ಇತ್ತೈಯನ್ನು ನೋಡಿ--ನೀನು ನಮ್ಮ ಸಂಗಡ ಬರುವದು ಯಾಕೆ? ನಿನ್ನ ಸ್ಥಳಕ್ಕೆ ಹಿಂದಿರುಗಿ ಹೋಗಿ ಅರಸನ ಸಂಗಡ ಇರು. ಯಾಕಂದರೆ ನೀನು ಸೆರೆಹಿಡಿಯಲ್ಪಟ್ಟವನಾದ ಅನ್ಯದೇಶದವನು.', 'ವಿಶ್ವ ಗೋ ಸಮ್ಮೇಳನದ ಅಂಗಳದಲ್ಲಿ ಏನೇನು ನಡೆದಿದೆ?', 'ಪ್ರಧಾನ ಮಂತ್ರಿ ನರೇಂದ್ರ ಮೋದಿ, ರಕ್ಷಣಾ ಸಚಿವ ರಾಜನಾಥ್ ಸಿಂಗ್ ಮತ್ತು ಕೇಂದ್ರ ಗೃಹ ಸಚಿವ ಅಮಿತ್ ಷಾ ಅವರು ಮಸೂದೆಯ ಬಗ್ಗೆ ಸಾರ್ವಜನಿಕ ಚರ್ಚೆ ಗೆ ಬರುವಂತೆ ಸಂಘ ಸವಾಲು ಹಾಕಿದೆ.')]\n",
            "[('It has been under discussion for a long time.', 'Buses cannot get there.', 'Why then this tradition was not thought of?'), ('ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.', 'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.', 'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?')]\n"
          ]
        }
      ],
      "source": [
        "# Print the first few batches to see the structure.\n",
        "# Each batch is a list of 2 tuples: batch[0] = English sentences, batch[1] = Kannada sentences.\n",
        "# Each contains `batch_size` (3) sentences.\n",
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Tokenization — Converting Characters to Integer Indices\n",
        "\n",
        "**Tokenization** translates each character in a sentence into its corresponding integer index from the vocabulary dictionary. The process:\n",
        "\n",
        "1. **Map each character** → its integer index using `language_to_index` dictionary.\n",
        "2. **Optionally prepend START token** — Used for Kannada (target) to give the decoder an initial input.\n",
        "3. **Optionally append END token** — Marks where the sentence ends so the model learns to stop generating.\n",
        "4. **Pad with PADDING tokens** — Fill remaining positions up to `max_sequence_length` so all sequences have equal length.\n",
        "5. **Convert to PyTorch tensor** — Ready for input to the model's embedding layer.\n",
        "\n",
        "### Token usage by language:\n",
        "| Token | English (Encoder input) | Kannada (Decoder input) |\n",
        "|-------|------------------------|------------------------|\n",
        "| START | **No** — entire sentence processed at once | **Yes** — needed as initial input for generation |\n",
        "| END   | **No** | **Yes** — model learns when to stop |\n",
        "| PADDING | **Yes** — to fill to fixed length | **Yes** — to fill to fixed length |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf9LqFyjJFGs"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence, language_to_index, start_token=True, end_token=True):\n",
        "    \"\"\"Convert a sentence string into a fixed-length tensor of integer indices.\n",
        "    \n",
        "    Args:\n",
        "        sentence: Raw sentence string (e.g., \"Hello\" or \"ನಮಸ್ಕಾರ\")\n",
        "        language_to_index: Dict mapping each character → integer index\n",
        "        start_token: If True, prepend the START token index (used for Kannada/target)\n",
        "        end_token: If True, append the END token index\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor of shape (max_sequence_length,) with integer token indices\n",
        "    \"\"\"\n",
        "    # Step 1: Convert each character to its integer index\n",
        "    sentence_word_indicies = [language_to_index[token] for token in list(sentence)]\n",
        "    \n",
        "    # Step 2: Optionally add START token at the beginning (for decoder/target sentences)\n",
        "    if start_token:\n",
        "        sentence_word_indicies.insert(0, language_to_index[START_TOKEN])\n",
        "    \n",
        "    # Step 3: Optionally add END token at the end (signals sentence completion)\n",
        "    if end_token:\n",
        "        sentence_word_indicies.append(language_to_index[END_TOKEN])\n",
        "    \n",
        "    # Step 4: Pad with PADDING tokens to reach max_sequence_length\n",
        "    # This ensures all sequences in a batch have the same length\n",
        "    for _ in range(len(sentence_word_indicies), max_sequence_length):\n",
        "        sentence_word_indicies.append(language_to_index[PADDING_TOKEN])\n",
        "    \n",
        "    return torch.tensor(sentence_word_indicies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqKgDTgHpZx7",
        "outputId": "88972da2-cfbc-4b50-b6c7-8eda5decbc9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('It has been under discussion for a long time.',\n",
              "  'Buses cannot get there.',\n",
              "  'Why then this tradition was not thought of?'),\n",
              " ('ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.',\n",
              "  'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.',\n",
              "  'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?')]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the current batch — it's a tuple: (english_sentences_list, kannada_sentences_list)\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf3oWss4po_n",
        "outputId": "91aa4f14-5d7f-4005-ba6c-310adad7cdfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('It has been under discussion for a long time.',\n",
              " 'Buses cannot get there.',\n",
              " 'Why then this tradition was not thought of?')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access one sentence from the batch (batch[0] = English, batch[1] = Kannada)\n",
        "batch[sentence_num]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize a Batch of Sentences\n",
        "\n",
        "For each sentence pair in the batch:\n",
        "- **English** (encoder input): Tokenized **without** START or END tokens — the encoder processes the entire source sentence at once.\n",
        "- **Kannada** (decoder input): Tokenized **with** both START and END tokens — the decoder needs START as initial input and END to learn when to stop generating.\n",
        "\n",
        "The tokenized sequences are stacked into tensors of shape `(batch_size, max_sequence_length)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ng2eqyKJH9-"
      },
      "outputs": [],
      "source": [
        "# Tokenize all sentences in the current batch\n",
        "eng_tokenized, kn_tokenized = [], []\n",
        "for sentence_num in range(batch_size):\n",
        "    eng_sentence, kn_sentence = batch[0][sentence_num], batch[1][sentence_num]\n",
        "    \n",
        "    # English (encoder): NO start/end tokens — encoder sees the full source sentence at once\n",
        "    eng_tokenized.append( tokenize(eng_sentence, english_to_index, start_token=False, end_token=False) )\n",
        "    \n",
        "    # Kannada (decoder): WITH start/end tokens — decoder generates one token at a time\n",
        "    kn_tokenized.append( tokenize(kn_sentence, kannada_to_index, start_token=True, end_token=True) )\n",
        "\n",
        "# Stack individual sentence tensors into batch tensors: shape (batch_size, max_sequence_length)\n",
        "eng_tokenized = torch.stack(eng_tokenized)\n",
        "kn_tokenized = torch.stack(kn_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYIMgY4eqYtF",
        "outputId": "9afd4417-8c39-4d6e-face-5fad6b5ccb24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[41, 84,  1, 72, 65, 83,  1, 66, 69, 69, 78,  1, 85, 78, 68, 69, 82,  1,\n",
              "         68, 73, 83, 67, 85, 83, 83, 73, 79, 78,  1, 70, 79, 82,  1, 65,  1, 76,\n",
              "         79, 78, 71,  1, 84, 73, 77, 69, 15, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95],\n",
              "        [34, 85, 83, 69, 83,  1, 67, 65, 78, 78, 79, 84,  1, 71, 69, 84,  1, 84,\n",
              "         72, 69, 82, 69, 15, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95],\n",
              "        [55, 72, 89,  1, 84, 72, 69, 78,  1, 84, 72, 73, 83,  1, 84, 82, 65, 68,\n",
              "         73, 84, 73, 79, 78,  1, 87, 65, 83,  1, 78, 79, 84,  1, 84, 72, 79, 85,\n",
              "         71, 72, 84,  1, 79, 70, 31, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
              "         95, 95]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the tokenized English batch — shape: (3, 200)\n",
        "# Each row is a sentence: [char_indices..., PADDING_TOKEN_indices...]\n",
        "# No START or END tokens present for English\n",
        "eng_tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Attention Masking — Controlling What the Model Can \"See\"\n",
        "\n",
        "Masking is critical in Transformers to ensure the model only attends to valid, appropriate tokens. Three types of masks are created:\n",
        "\n",
        "### 1. Encoder Self-Attention Mask (Padding Mask)\n",
        "- Prevents the encoder from attending to **padding tokens**.\n",
        "- Shape: `(batch_size, max_seq_len, max_seq_len)` — one 2D mask per sentence.\n",
        "- Padded positions are set to a large negative number; valid positions are 0.\n",
        "\n",
        "### 2. Decoder Self-Attention Mask (Look-Ahead + Padding Mask)\n",
        "- **Look-ahead mask**: An upper-triangular matrix that prevents the decoder from \"seeing\" **future tokens** during training. This is essential for autoregressive generation — the model should only use current and previous tokens to predict the next one.\n",
        "- Combined with padding mask to also block attention on padded positions.\n",
        "\n",
        "### 3. Decoder Cross-Attention Mask (Padding Mask)\n",
        "- Applied when the decoder attends to the encoder's output.\n",
        "- Prevents the decoder from attending to **padded positions** in the source (English) sentence.\n",
        "\n",
        "### Why use large negative numbers instead of literal `-inf`?\n",
        "- During softmax: `softmax(0) → passes value`, `softmax(-∞) → 0 (blocks value)`.\n",
        "- Using literal `-inf` can cause **NaN** values if an entire row is masked (softmax of all `-inf`).\n",
        "- Instead, we use `-1e9` (negative one billion) — large enough to effectively zero out attention, but numerically stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu581-voJPvp"
      },
      "outputs": [],
      "source": [
        "# Large negative value used as mask value instead of -inf to avoid NaN in softmax.\n",
        "# softmax(-1e9) ≈ 0 (effectively blocks attention), but won't produce NaN like -inf would.\n",
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    \"\"\"Create attention masks for the encoder and decoder.\n",
        "    \n",
        "    Args:\n",
        "        eng_batch: List of raw English sentence strings (source)\n",
        "        kn_batch: List of raw Kannada sentence strings (target)\n",
        "    \n",
        "    Returns:\n",
        "        encoder_self_attention_mask: Blocks padding in encoder self-attention\n",
        "        decoder_self_attention_mask: Blocks future tokens + padding in decoder self-attention\n",
        "        decoder_cross_attention_mask: Blocks padding when decoder attends to encoder output\n",
        "    \"\"\"\n",
        "    num_sentences = len(eng_batch)\n",
        "    \n",
        "    # --- Look-Ahead Mask (for decoder self-attention) ---\n",
        "    # Upper triangular matrix of True values: prevents attending to future positions.\n",
        "    # Example for seq_len=4:  [[F, T, T, T],   (position 0 can only see position 0)\n",
        "    #                          [F, F, T, T],   (position 1 can see 0 and 1)\n",
        "    #                          [F, F, F, T],   (position 2 can see 0, 1, and 2)\n",
        "    #                          [F, F, F, F]]   (position 3 can see all previous)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)  # Upper triangle (above main diagonal)\n",
        "    \n",
        "    # --- Initialize padding masks as all False (no masking) ---\n",
        "    # Shape: (batch_size, max_seq_len, max_seq_len) — one 2D attention matrix per sentence\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      \n",
        "      # Indices of positions that should be masked (padding positions).\n",
        "      # +1 accounts for the END token that will be added during tokenization.\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      \n",
        "      # Encoder padding mask: block attention TO and FROM padding positions\n",
        "      # Setting both row and column ensures padded tokens neither attend nor are attended to\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True   # No token attends TO padding\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True   # Padding doesn't attend TO any token\n",
        "      \n",
        "      # Decoder self-attention padding mask: same logic for Kannada padding positions\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      \n",
        "      # Decoder cross-attention padding mask: decoder should not attend to English padding\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    # Convert boolean masks to float masks: True → NEG_INFTY (blocked), False → 0 (allowed)\n",
        "    # After softmax: 0 → weight of 1 (passes), NEG_INFTY → weight of ~0 (blocked)\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    \n",
        "    # Decoder self-attention: combine look-ahead mask AND padding mask.\n",
        "    # A position is masked if EITHER the look-ahead OR padding mask is True.\n",
        "    # The '+' acts as logical OR for boolean tensors.\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    \n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    \n",
        "    # Debug: print mask shapes and a 10x10 corner to visualize the pattern\n",
        "    print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}: {encoder_self_attention_mask[0, :10, :10]}\")\n",
        "    print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}: {decoder_self_attention_mask[0, :10, :10]}\")\n",
        "    print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}: {decoder_cross_attention_mask[0, :10, :10]}\")\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY1xAivZJWOx",
        "outputId": "893b5441-d1db-44c0-aff3-8093a9756154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder_self_attention_mask torch.Size([3, 200, 200]): tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "decoder_self_attention_mask torch.Size([3, 200, 200]): tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
            "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
            "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "decoder_cross_attention_mask torch.Size([3, 200, 200]): tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]],\n",
              " \n",
              "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]],\n",
              " \n",
              "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]]]),\n",
              " tensor([[[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]],\n",
              " \n",
              "         [[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]],\n",
              " \n",
              "         [[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]]]),\n",
              " tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]],\n",
              " \n",
              "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]],\n",
              " \n",
              "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          ...,\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09],\n",
              "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
              "           -1.0000e+09, -1.0000e+09]]]))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate and inspect the three attention masks for the current batch.\n",
        "# The 10x10 slices show:\n",
        "#   - 0.0 = \"allowed to attend\" (will pass through softmax normally)\n",
        "#   - -1e9 = \"blocked\" (will be zeroed out by softmax)\n",
        "create_masks(batch[0], batch[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Sentence Embedding Class — Putting It All Together\n",
        "\n",
        "The `SentenceEmbedding` class integrates all preprocessing steps into a single PyTorch module that will be used inside the Transformer:\n",
        "\n",
        "1. **Batch Tokenization** — Converts a batch of raw sentence strings into integer index tensors (with optional START/END tokens and padding).\n",
        "2. **Embedding Lookup** — Maps each integer index to a dense vector of size `d_model` using `nn.Embedding`.\n",
        "3. **Positional Encoding** — Adds sinusoidal position information so the model knows the order of tokens (since self-attention is position-agnostic).\n",
        "4. **Dropout** — Randomly zeroes some values during training to prevent overfitting.\n",
        "\n",
        "This class will be instantiated separately for the **encoder** (English) and **decoder** (Kannada), each with its own vocabulary and embedding weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L770BHODEpvw"
      },
      "outputs": [],
      "source": [
        "class SentenceEmbedding(nn.Module):\n",
        "    \"\"\"Converts raw sentence strings into dense vector representations for the Transformer.\n",
        "    \n",
        "    Pipeline: Raw text → Tokenize → Embedding lookup → Add positional encoding → Dropout\n",
        "    \n",
        "    This module is used for BOTH encoder (English) and decoder (Kannada) inputs,\n",
        "    each instantiated with its own vocabulary and index mappings.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_sequence_length: Fixed length all sequences are padded/truncated to (200)\n",
        "            d_model: Dimensionality of embeddings (e.g., 512 in original Transformer)\n",
        "            language_to_index: Dict mapping characters → integer indices for this language\n",
        "            START_TOKEN, END_TOKEN, PADDING_TOKEN: Special token strings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)          # Total number of unique tokens in this language\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)  # Learnable lookup table: index → d_model vector\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)  # Sinusoidal position info\n",
        "        self.dropout = nn.Dropout(p=0.1)  # 10% dropout for regularization\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "    \n",
        "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
        "        \"\"\"Convert a batch of raw sentence strings into a padded integer tensor.\n",
        "        \n",
        "        Args:\n",
        "            batch: List of sentence strings\n",
        "            start_token: Whether to prepend START token (True for Kannada decoder input)\n",
        "            end_token: Whether to append END token\n",
        "        \n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, max_sequence_length) on the appropriate device\n",
        "        \"\"\"\n",
        "\n",
        "        def tokenize(sentence, start_token=True, end_token=True):\n",
        "            \"\"\"Tokenize a single sentence into a fixed-length list of integer indices.\"\"\"\n",
        "            # Map each character to its vocabulary index\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            \n",
        "            # Optionally add START token at the beginning\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            \n",
        "            # Optionally add END token at the end\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            \n",
        "            # Pad remaining positions with PADDING token to reach fixed length\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            \n",
        "            return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "        # Tokenize each sentence in the batch and stack into a 2D tensor\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = torch.stack(tokenized)  # Shape: (batch_size, max_sequence_length)\n",
        "        return tokenized.to(get_device())   # Move to GPU if available\n",
        "    \n",
        "    def forward(self, x, end_token=True):\n",
        "        \"\"\"Forward pass: raw sentences → embedded + positionally-encoded vectors.\n",
        "        \n",
        "        Args:\n",
        "            x: List of raw sentence strings (one batch)\n",
        "            end_token: Whether to add END token during tokenization\n",
        "        \n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, max_sequence_length, d_model)\n",
        "        \"\"\"\n",
        "        x = self.batch_tokenize(x, end_token)        # (batch, seq_len) — integer indices\n",
        "        x = self.embedding(x)                          # (batch, seq_len, d_model) — dense vectors\n",
        "        pos = self.position_encoder().to(get_device()) # (seq_len, d_model) — position information\n",
        "        x = self.dropout(x + pos)                      # Add position info + apply dropout\n",
        "        return x"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
