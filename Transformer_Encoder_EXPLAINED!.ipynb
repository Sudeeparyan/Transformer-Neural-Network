{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Encoder — Full Implementation in PyTorch\n",
        "\n",
        "This notebook implements the **Transformer Encoder** from scratch using PyTorch.\n",
        "\n",
        "## Architecture Overview\n",
        "The encoder consists of **N stacked identical layers**, each containing:\n",
        "1. **Multi-Head Self-Attention** — allows each token to attend to every other token in the sequence\n",
        "2. **Add & Layer Normalization** — residual connection followed by normalization\n",
        "3. **Position-wise Feed-Forward Network (FFN)** — two linear layers with ReLU activation\n",
        "4. **Add & Layer Normalization** — another residual + normalization\n",
        "\n",
        "## Key Hyperparameters\n",
        "| Parameter | Value | Description |\n",
        "|-----------|-------|-------------|\n",
        "| `d_model` | 512 | Embedding dimension for each token |\n",
        "| `num_heads` | 8 | Number of parallel attention heads |\n",
        "| `head_dim` | 64 | Dimension per head (512 / 8) |\n",
        "| `ffn_hidden` | 2048 | Hidden layer size in the FFN |\n",
        "| `num_layers` | 5 | Number of stacked encoder layers |\n",
        "| `drop_prob` | 0.1 | Dropout probability (regularization) |\n",
        "| `batch_size` | 30 | Number of sentences per mini-batch |\n",
        "| `max_sequence_length` | 200 | Fixed sequence length (padded) |\n",
        "\n",
        "## Tensor Shape Throughout the Encoder\n",
        "The input and output of every encoder layer is **[batch_size, max_sequence_length, d_model]** = **[30, 200, 512]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n",
        "- **torch**: The core PyTorch library for tensor operations and automatic differentiation.\n",
        "- **math**: Python's built-in math module — used here for `sqrt` in scaled dot-product attention.\n",
        "- **nn (torch.nn)**: Provides neural network building blocks like `Linear`, `Module`, `Parameter`, `Dropout`, etc.\n",
        "- **F (torch.nn.functional)**: Functional API for operations like `softmax` that don't have learnable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdTblxTF_byq"
      },
      "outputs": [],
      "source": [
        "import torch                        # Core PyTorch library for tensor computation & GPU acceleration\n",
        "import math                         # Python math library — we need sqrt() for scaling in attention\n",
        "from torch import nn                # Neural network module: layers (Linear, Dropout), base class (Module), etc.\n",
        "import torch.nn.functional as F     # Functional API: stateless ops like softmax, relu (no learnable params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define All Encoder Components\n",
        "\n",
        "This cell defines **5 components** that build up the Transformer Encoder:\n",
        "\n",
        "1. **`scaled_dot_product()`** — Core attention formula: $\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "2. **`MultiHeadAttention`** — Splits attention into multiple parallel heads for richer representations\n",
        "3. **`LayerNormalization`** — Normalizes across the embedding dimension with learnable γ and β\n",
        "4. **`PositionwiseFeedForward`** — Two-layer MLP (512→2048→512) applied independently to each position\n",
        "5. **`EncoderLayer`** — One complete encoder block (attention → add & norm → FFN → add & norm)\n",
        "6. **`Encoder`** — Stacks N encoder layers sequentially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYbIgWZ3_JXg"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1. SCALED DOT-PRODUCT ATTENTION (the core math behind attention)\n",
        "# ============================================================================\n",
        "# Formula:  Attention(Q, K, V) = softmax( (Q @ K^T) / sqrt(d_k) ) @ V\n",
        "#\n",
        "# Why scale by sqrt(d_k)?\n",
        "#   - The dot products Q·K grow in magnitude as d_k increases.\n",
        "#   - Large values push softmax into regions with tiny gradients (saturation).\n",
        "#   - Dividing by sqrt(d_k) keeps the variance ≈ 1, giving healthier gradients.\n",
        "#\n",
        "# Parameters:\n",
        "#   q : Query tensor  — shape [batch_size, num_heads, seq_len, head_dim]  (what am I looking for?)\n",
        "#   k : Key tensor    — shape [batch_size, num_heads, seq_len, head_dim]  (what do I contain?)\n",
        "#   v : Value tensor  — shape [batch_size, num_heads, seq_len, head_dim]  (what information do I carry?)\n",
        "#   mask : Optional tensor to block certain positions (used in decoder, not needed in encoder)\n",
        "#\n",
        "# Returns:\n",
        "#   values    — weighted sum of V vectors  [batch_size, num_heads, seq_len, head_dim]\n",
        "#   attention — attention weight matrix     [batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    # d_k = dimension of each head (e.g., 64 when d_model=512 and num_heads=8)\n",
        "    d_k = q.size()[-1]\n",
        "\n",
        "    # Step A: Compute raw attention scores by taking dot product of Q and K^T\n",
        "    # q shape:        [batch, heads, seq_len, head_dim]   e.g. [30, 8, 200, 64]\n",
        "    # k.transpose:    [batch, heads, head_dim, seq_len]   e.g. [30, 8, 64, 200]\n",
        "    # Result \"scaled\": [batch, heads, seq_len, seq_len]   e.g. [30, 8, 200, 200]\n",
        "    # Each entry (i, j) = how much token i should attend to token j\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    print(f\"scaled.size() : {scaled.size()}\")\n",
        "\n",
        "    # Step B: (Optional) Apply mask — used in decoder to prevent attending to future tokens\n",
        "    # In the ENCODER self-attention, mask is None because every token can see every other token\n",
        "    if mask is not None:\n",
        "        print(f\"-- ADDING MASK of shape {mask.size()} --\")\n",
        "        # Mask positions are typically filled with -inf so that softmax maps them to ~0\n",
        "        # Broadcasting add: only the last N dimensions need to match\n",
        "        scaled += mask\n",
        "\n",
        "    # Step C: Apply softmax along the last dimension (across keys for each query)\n",
        "    # Converts raw scores into probabilities that sum to 1 for each query position\n",
        "    # Shape stays [batch, heads, seq_len, seq_len]\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "\n",
        "    # Step D: Multiply attention weights by V to get the weighted combination of values\n",
        "    # attention: [batch, heads, seq_len, seq_len]   e.g. [30, 8, 200, 200]\n",
        "    # v:         [batch, heads, seq_len, head_dim]   e.g. [30, 8, 200, 64]\n",
        "    # Result:    [batch, heads, seq_len, head_dim]   e.g. [30, 8, 200, 64]\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. MULTI-HEAD ATTENTION\n",
        "# ============================================================================\n",
        "# Instead of performing a single attention function with d_model-dimensional keys,\n",
        "# values and queries, it is beneficial to split them into 'num_heads' parallel\n",
        "# attention operations, each with dimension head_dim = d_model / num_heads.\n",
        "#\n",
        "# Why multiple heads?\n",
        "#   - Each head can learn to attend to DIFFERENT types of relationships:\n",
        "#     e.g., one head might focus on syntactic relations, another on semantic similarity.\n",
        "#   - The outputs of all heads are concatenated and linearly projected back.\n",
        "#\n",
        "# Tensor flow through this class:\n",
        "#   Input x:           [batch_size, seq_len, d_model]         e.g. [30, 200, 512]\n",
        "#   After qkv_layer:   [batch_size, seq_len, 3 * d_model]    e.g. [30, 200, 1536]\n",
        "#   Reshape:           [batch_size, seq_len, num_heads, 3*head_dim] e.g. [30, 200, 8, 192]\n",
        "#   Permute:           [batch_size, num_heads, seq_len, 3*head_dim] e.g. [30, 8, 200, 192]\n",
        "#   Split into Q,K,V:  each [batch_size, num_heads, seq_len, head_dim] e.g. [30, 8, 200, 64]\n",
        "#   Attention output:  [batch_size, num_heads, seq_len, head_dim]      e.g. [30, 8, 200, 64]\n",
        "#   Reshape (concat):  [batch_size, seq_len, d_model]                  e.g. [30, 200, 512]\n",
        "#   Final linear:      [batch_size, seq_len, d_model]                  e.g. [30, 200, 512]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model        # Total embedding dimension (e.g., 512)\n",
        "        self.num_heads = num_heads     # Number of parallel attention heads (e.g., 8)\n",
        "        self.head_dim = d_model // num_heads  # Dimension per head: 512 / 8 = 64\n",
        "\n",
        "        # Single linear layer that projects input into Q, K, and V simultaneously\n",
        "        # Input:  [batch, seq_len, 512]\n",
        "        # Output: [batch, seq_len, 1536]  (512*3 = 1536, for Q, K, V concatenated)\n",
        "        # This is more efficient than having 3 separate linear layers\n",
        "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
        "\n",
        "        # Final linear projection after concatenating all heads back together\n",
        "        # Input:  [batch, seq_len, 512]  (concatenated heads)\n",
        "        # Output: [batch, seq_len, 512]\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x shape: [batch_size, max_sequence_length, d_model] = [30, 200, 512]\n",
        "        batch_size, max_sequence_length, d_model = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "\n",
        "        # Project input into Q, K, V all at once\n",
        "        # [30, 200, 512] → [30, 200, 1536]\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "        # Reshape to separate the heads:\n",
        "        # [30, 200, 1536] → [30, 200, 8, 192]\n",
        "        # 192 = 3 * head_dim = 3 * 64 (still contains Q, K, V concatenated per head)\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "        # Move the heads dimension before sequence length for parallel computation\n",
        "        # [30, 200, 8, 192] → [30, 8, 200, 192]\n",
        "        # Now each head can independently process all 200 sequence positions\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "        # Split the last dimension into 3 equal chunks: Q, K, V\n",
        "        # Each: [30, 8, 200, 64]  — 8 heads, each attending with 64-dim vectors\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "\n",
        "        # Compute scaled dot-product attention for all heads in parallel\n",
        "        # values:    [30, 8, 200, 64]  — contextualized representations per head\n",
        "        # attention: [30, 8, 200, 200] — attention weight matrices per head\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "\n",
        "        # Concatenate all heads back together:\n",
        "        # [30, 8, 200, 64] → [30, 200, 512]  (8 heads × 64 dims = 512)\n",
        "        # This merges the information learned by each head into a single representation\n",
        "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "\n",
        "        # Final linear projection: mix information across heads\n",
        "        # [30, 200, 512] → [30, 200, 512]\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. LAYER NORMALIZATION\n",
        "# ============================================================================\n",
        "# Normalizes the input across the LAST dimension(s) (the embedding dimension).\n",
        "# This is different from Batch Normalization, which normalizes across the batch dimension.\n",
        "#\n",
        "# Why Layer Norm?\n",
        "#   - Stabilizes training by ensuring each layer receives inputs with consistent\n",
        "#     mean (~0) and variance (~1), reducing \"internal covariate shift\".\n",
        "#   - Works well with variable-length sequences (unlike Batch Norm).\n",
        "#\n",
        "# Formula:\n",
        "#   y = (x - mean) / std\n",
        "#   out = gamma * y + beta\n",
        "#\n",
        "#   - gamma (γ): learnable scale parameter, initialized to 1  (shape: [512])\n",
        "#   - beta  (β): learnable shift parameter, initialized to 0  (shape: [512])\n",
        "#   - These allow the model to undo the normalization if that's beneficial.\n",
        "#\n",
        "# Input shape:  [batch_size, seq_len, d_model]  e.g. [30, 200, 512]\n",
        "# Output shape: [batch_size, seq_len, d_model]  e.g. [30, 200, 512]  (unchanged)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape = parameters_shape  # Shape of the dimension(s) to normalize over, e.g. [512]\n",
        "        self.eps = eps  # Small constant to prevent division by zero in std calculation\n",
        "\n",
        "        # Learnable parameters:\n",
        "        # gamma (scale): initialized to ones — starts as identity transform\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        # beta (shift): initialized to zeros — starts with no bias\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs shape: [30, 200, 512]\n",
        "\n",
        "        # Build the list of dimensions to normalize over\n",
        "        # For parameters_shape=[512], dims=[-1], meaning normalize over the last (embedding) dimension\n",
        "        # For each token independently, we compute mean and variance across all 512 features\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "\n",
        "        # Compute mean across the embedding dimension, keeping dims for broadcasting\n",
        "        # mean shape: [30, 200, 1] — one mean value per token\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        print(f\"Mean ({mean.size()})\")\n",
        "\n",
        "        # Compute variance: E[(x - mean)^2]\n",
        "        # var shape: [30, 200, 1]\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "\n",
        "        # Standard deviation (with epsilon for numerical stability)\n",
        "        # std shape: [30, 200, 1]\n",
        "        std = (var + self.eps).sqrt()\n",
        "        print(f\"Standard Deviation  ({std.size()})\")\n",
        "\n",
        "        # Normalize: zero mean, unit variance\n",
        "        # y shape: [30, 200, 512]\n",
        "        y = (inputs - mean) / std\n",
        "        print(f\"y: {y.size()}\")\n",
        "\n",
        "        # Scale and shift with learnable parameters\n",
        "        # gamma [512] broadcasts across batch and seq dimensions\n",
        "        # beta  [512] broadcasts across batch and seq dimensions\n",
        "        # out shape: [30, 200, 512]\n",
        "        out = self.gamma * y + self.beta\n",
        "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
        "        print(f\"out: {out.size()}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. POSITION-WISE FEED-FORWARD NETWORK (FFN)\n",
        "# ============================================================================\n",
        "# A simple two-layer fully connected network applied to each position INDEPENDENTLY.\n",
        "# \"Position-wise\" means the same MLP is applied to every token in the sequence separately.\n",
        "#\n",
        "# Architecture:\n",
        "#   Linear1:  512 → 2048     (expand to a higher-dimensional space)\n",
        "#   ReLU:     activation      (introduce non-linearity)\n",
        "#   Dropout:  regularization  (randomly zero out neurons during training)\n",
        "#   Linear2:  2048 → 512     (project back to original dimension)\n",
        "#\n",
        "# Why expand then contract?\n",
        "#   - The wider hidden layer (2048) gives the network more capacity to learn\n",
        "#     complex feature transformations, while projecting back to 512 keeps\n",
        "#     the representation size consistent for the next encoder layer.\n",
        "#\n",
        "# Input shape:  [30, 200, 512]\n",
        "# Output shape: [30, 200, 512]  (same!)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)   # 512 → 2048 (expansion)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)    # 2048 → 512 (compression)\n",
        "        self.relu = nn.ReLU()                         # Non-linear activation function\n",
        "        self.dropout = nn.Dropout(p=drop_prob)         # Dropout for regularization (10% neurons zeroed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [30, 200, 512]\n",
        "\n",
        "        # Expand: [30, 200, 512] → [30, 200, 2048]\n",
        "        x = self.linear1(x)\n",
        "        print(f\"x after first linear layer: {x.size()}\")\n",
        "\n",
        "        # Apply ReLU: max(0, x) — introduces non-linearity so the network\n",
        "        # can learn more complex functions than just linear transformations\n",
        "        # Shape unchanged: [30, 200, 2048]\n",
        "        x = self.relu(x)\n",
        "        print(f\"x after activation: {x.size()}\")\n",
        "\n",
        "        # Dropout: randomly set ~10% of values to 0 during training\n",
        "        # This prevents the network from relying too heavily on any single neuron\n",
        "        # Shape unchanged: [30, 200, 2048]\n",
        "        x = self.dropout(x)\n",
        "        print(f\"x after dropout: {x.size()}\")\n",
        "\n",
        "        # Compress back: [30, 200, 2048] → [30, 200, 512]\n",
        "        x = self.linear2(x)\n",
        "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. ENCODER LAYER (one complete block)\n",
        "# ============================================================================\n",
        "# Each encoder layer performs the following sequence:\n",
        "#\n",
        "#   ┌─────────────────────────────────────────────────────┐\n",
        "#   │  Input x  [30, 200, 512]                            │\n",
        "#   │     │                                               │\n",
        "#   │     ├──────── (save as residual_x) ────────┐        │\n",
        "#   │     ▼                                      │        │\n",
        "#   │  Multi-Head Self-Attention                  │        │\n",
        "#   │     ▼                                      │        │\n",
        "#   │  Dropout                                   │        │\n",
        "#   │     ▼                                      │        │\n",
        "#   │  Add (x + residual_x)  ← residual connection       │\n",
        "#   │     ▼                                               │\n",
        "#   │  Layer Normalization                                │\n",
        "#   │     │                                               │\n",
        "#   │     ├──────── (save as residual_x) ────────┐        │\n",
        "#   │     ▼                                      │        │\n",
        "#   │  Feed-Forward Network                      │        │\n",
        "#   │     ▼                                      │        │\n",
        "#   │  Dropout                                   │        │\n",
        "#   │     ▼                                      │        │\n",
        "#   │  Add (x + residual_x)  ← residual connection       │\n",
        "#   │     ▼                                               │\n",
        "#   │  Layer Normalization                                │\n",
        "#   │     ▼                                               │\n",
        "#   │  Output [30, 200, 512]                              │\n",
        "#   └─────────────────────────────────────────────────────┘\n",
        "#\n",
        "# Residual connections:\n",
        "#   - The original input is ADDED to the output of each sub-layer before normalization.\n",
        "#   - This helps gradients flow directly through the network during backpropagation,\n",
        "#     solving the \"vanishing gradient\" problem in deep networks.\n",
        "#   - Intuition: the layer only needs to learn the RESIDUAL (what to add/change),\n",
        "#     not the entire transformation from scratch.\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Sub-layer 1: Multi-Head Self-Attention\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        # Layer Norm after attention (normalizes over embedding dim [512])\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        # Dropout after attention\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        # Sub-layer 2: Position-wise Feed-Forward Network\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        # Layer Norm after FFN\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        # Dropout after FFN\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [30, 200, 512]\n",
        "\n",
        "        # ---- Sub-layer 1: Multi-Head Self-Attention + Add & Norm ----\n",
        "        residual_x = x  # Save input for residual connection\n",
        "        print(\"------- ATTENTION 1 ------\")\n",
        "        x = self.attention(x, mask=None)  # Self-attention (mask=None for encoder)\n",
        "        print(\"------- DROPOUT 1 ------\")\n",
        "        x = self.dropout1(x)              # Apply dropout for regularization\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
        "        x = self.norm1(x + residual_x)    # Add residual, then normalize\n",
        "\n",
        "        # ---- Sub-layer 2: Feed-Forward Network + Add & Norm ----\n",
        "        residual_x = x  # Save input for second residual connection\n",
        "        print(\"------- ATTENTION 2 ------\")\n",
        "        x = self.ffn(x)                   # Feed-Forward Network (NOT attention — label is misleading)\n",
        "        print(\"------- DROPOUT 2 ------\")\n",
        "        x = self.dropout2(x)              # Apply dropout\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
        "        x = self.norm2(x + residual_x)    # Add residual, then normalize\n",
        "\n",
        "        return x  # Output shape: [30, 200, 512] — same as input!\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. ENCODER (stack of N encoder layers)\n",
        "# ============================================================================\n",
        "# The full encoder simply stacks N identical EncoderLayer modules sequentially.\n",
        "# Data flows through each layer one after another:\n",
        "#   Input → EncoderLayer_1 → EncoderLayer_2 → ... → EncoderLayer_N → Output\n",
        "#\n",
        "# nn.Sequential: PyTorch container that chains modules — calling forward()\n",
        "# on the Sequential passes input through each child module in order.\n",
        "#\n",
        "# Input shape:  [30, 200, 512]\n",
        "# Output shape: [30, 200, 512]  (unchanged through all N layers)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super().__init__()\n",
        "        # Create a list of `num_layers` identical encoder layers and wrap in Sequential\n",
        "        # The * unpacks the list so each EncoderLayer is a separate argument to Sequential\n",
        "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                     for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through all encoder layers sequentially\n",
        "        # Each layer: attention → add&norm → FFN → add&norm\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Set Hyperparameters & Instantiate the Encoder\n",
        "\n",
        "These are the key settings that control the architecture:\n",
        "\n",
        "| Parameter | Value | Why this value? |\n",
        "|-----------|-------|-----------------|\n",
        "| `d_model = 512` | Embedding dimension | Standard from \"Attention Is All You Need\" paper |\n",
        "| `num_heads = 8` | Parallel attention heads | 512/8 = 64 dims per head — good balance |\n",
        "| `drop_prob = 0.1` | 10% dropout | Mild regularization to prevent overfitting |\n",
        "| `batch_size = 30` | Mini-batch size | Trade-off between training speed and memory |\n",
        "| `max_sequence_length = 200` | Padded sequence length | All sentences padded/truncated to same length |\n",
        "| `ffn_hidden = 2048` | FFN hidden layer size | 4× the embedding dim (standard ratio) |\n",
        "| `num_layers = 5` | Encoder depth | 5 stacked identical layers |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FzFhz5N_5qd"
      },
      "outputs": [],
      "source": [
        "# ---- Hyperparameters (from the original Transformer paper) ----\n",
        "d_model = 512              # Embedding dimension: each token is a 512-dimensional vector\n",
        "num_heads = 8              # Number of attention heads (each head sees 512/8 = 64 dimensions)\n",
        "drop_prob = 0.1            # Dropout rate: 10% of neurons randomly turned off during training\n",
        "batch_size = 30            # Number of sentences processed together in one forward pass\n",
        "max_sequence_length = 200  # All input sequences padded/truncated to this fixed length\n",
        "ffn_hidden = 2048          # Hidden size in feed-forward network (4x expansion of d_model)\n",
        "num_layers = 5             # Number of stacked encoder layers (deeper = more capacity)\n",
        "\n",
        "# ---- Instantiate the Encoder ----\n",
        "# This creates 5 encoder layers, each containing:\n",
        "#   - MultiHeadAttention (8 heads, 512 dims)\n",
        "#   - LayerNormalization (over 512 dims)\n",
        "#   - PositionwiseFeedForward (512 → 2048 → 512)\n",
        "#   - Dropout layers and residual connections\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run a Forward Pass Through the Encoder\n",
        "\n",
        "We create a **random input tensor** simulating a batch of 30 sentences, each with 200 tokens, each token represented as a 512-dimensional embedding (which would normally include positional encoding).\n",
        "\n",
        "**Expected output shape: [30, 200, 512]** — the encoder preserves the input dimensions exactly, but the values are now contextualized (each token's representation incorporates information from all other tokens via self-attention)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysrs-8UlADVd",
        "outputId": "505ca0b2-ac1f-4b52-f491-328ef2bf40b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ],
      "source": [
        "# Create a random input tensor to simulate token embeddings + positional encodings\n",
        "# Shape: [batch_size, max_sequence_length, d_model] = [30, 200, 512]\n",
        "# In a real scenario, this would come from:\n",
        "#   1. Token embeddings (learned lookup table mapping word IDs → 512-dim vectors)\n",
        "#   2. + Positional encodings (sinusoidal signals encoding each token's position)\n",
        "x = torch.randn((batch_size, max_sequence_length, d_model))\n",
        "\n",
        "# Pass through all 5 encoder layers\n",
        "# The input flows: Layer1 → Layer2 → Layer3 → Layer4 → Layer5\n",
        "# Each layer applies: Attention → Add&Norm → FFN → Add&Norm\n",
        "# Output shape: [30, 200, 512] — same as input, but now CONTEXTUALIZED\n",
        "# (each token's vector now contains information about all other tokens in the sequence)\n",
        "out = encoder(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwOOsnedJwaa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
