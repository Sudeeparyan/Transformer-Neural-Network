{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SvxpyUxQmqP"
      },
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "**Multi-Head Attention** is the key mechanism inside Transformers that allows the model to attend to different parts of the input sequence simultaneously from multiple \"perspectives.\"\n",
        "\n",
        "Instead of computing a single attention function, the model:\n",
        "1. **Splits** the Q, K, V vectors into multiple smaller \"heads\" (e.g., 8 heads of 64 dimensions each from a 512-dim vector).\n",
        "2. Each head **independently** computes scaled dot-product attention.\n",
        "3. The outputs of all heads are **concatenated** back into a single vector.\n",
        "4. A **linear layer** mixes information across heads to produce the final output.\n",
        "\n",
        "**Why multiple heads?** A single attention head might focus on one type of relationship (e.g., syntactic). Multiple heads can capture different types of relationships simultaneously (syntactic, semantic, positional, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqzOybgA6tDz"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Import libraries\n",
        "# -------------------------------------------------------------------\n",
        "import numpy as np               # For numerical operations (used in histogram plotting)\n",
        "import torch                     # PyTorch — the deep learning framework\n",
        "import torch.nn as nn            # Neural network modules (Linear layers, etc.)\n",
        "import torch.nn.functional as F  # Functional API — contains softmax, relu, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2NI4FSG8JY2"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Define input dimensions and create a random input tensor\n",
        "# -------------------------------------------------------------------\n",
        "# sequence_length = 4  → Number of tokens/words in the input sentence\n",
        "#                        Example: \"my name is Aj\" → 4 tokens\n",
        "# batch_size = 1       → Processing 1 sentence at a time\n",
        "#                        (In real training, batch_size is larger, e.g., 32 or 64)\n",
        "# input_dim = 512      → Dimension of each input word embedding\n",
        "#                        (Each word is represented as a 512-dimensional vector)\n",
        "# d_model = 512        → Internal model dimension (the Transformer's working dimension)\n",
        "#                        Often same as input_dim, but can differ\n",
        "sequence_length = 4\n",
        "batch_size = 1\n",
        "input_dim = 512\n",
        "d_model = 512\n",
        "\n",
        "# Create a random input tensor to simulate word embeddings\n",
        "# Shape: (batch_size, sequence_length, input_dim) = (1, 4, 512)\n",
        "#   - Dimension 0: batch (1 sentence)\n",
        "#   - Dimension 1: sequence (4 words)\n",
        "#   - Dimension 2: embedding (512-dim vector per word)\n",
        "# In a real Transformer, this would come from an embedding layer + positional encoding\n",
        "x = torch.randn( (batch_size, sequence_length, input_dim) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RII2sE0iuSh",
        "outputId": "adc44ac9-82c0-4fc1-f6ac-13da59a079e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify the input shape: should be (1, 4, 512)\n",
        "# (batch_size=1, sequence_length=4, input_dim=512)\n",
        "x.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xojRX8Q5jTfY"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Create a single linear layer to generate Q, K, V all at once\n",
        "# -------------------------------------------------------------------\n",
        "# Instead of 3 separate linear layers (one each for Q, K, V), we use ONE linear\n",
        "# layer that outputs 3× the model dimension. This is more efficient!\n",
        "#\n",
        "# nn.Linear(input_dim, 3 * d_model) = nn.Linear(512, 1536)\n",
        "#   - Input:  512-dim vector per word\n",
        "#   - Output: 1536-dim vector per word (which we'll split into Q, K, V later)\n",
        "#   - The 1536 = 3 × 512 → first 512 dims = Q, next 512 = K, last 512 = V\n",
        "#\n",
        "# This layer contains a learned weight matrix W of shape (512, 1536) + bias (1536)\n",
        "# Each word's embedding is multiplied by this weight matrix to produce its Q, K, V\n",
        "qkv_layer = nn.Linear(input_dim , 3 * d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnwj8NFTkWHC"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Pass the input through the QKV linear layer\n",
        "# -------------------------------------------------------------------\n",
        "# x shape:   (1, 4, 512)  → 1 batch, 4 words, 512-dim each\n",
        "# qkv shape: (1, 4, 1536) → 1 batch, 4 words, 1536-dim each\n",
        "#\n",
        "# For each word, the 1536-dim output contains:\n",
        "#   [0:512]    = Query vector  (what this word is looking for)\n",
        "#   [512:1024] = Key vector    (what this word offers for matching)\n",
        "#   [1024:1536]= Value vector  (the actual information this word carries)\n",
        "qkv = qkv_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HICI_ofJk66H",
        "outputId": "83151566-80b1-4718-cabb-137cdb3eaf09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 1536])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify QKV shape: should be (1, 4, 1536)\n",
        "# 1536 = 3 × 512  (concatenated Q, K, V for each word)\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Q2B8dXUlkkEE",
        "outputId": "fa91dc19-80e5-4ac8-b47b-41a1b36db856"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'qkv distribution')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Visualize the distribution of QKV values\n",
        "# -------------------------------------------------------------------\n",
        "# After the linear layer, the QKV values should roughly follow a normal distribution\n",
        "# (since our input was random normal and the linear layer has random weights)\n",
        "# This histogram helps verify that values are well-distributed and not saturated\n",
        "import matplotlib.pyplot as plt\n",
        "y_val = torch.histc(qkv, bins=200, min=-3, max=3)  # Count values in 200 bins from -3 to 3\n",
        "x_val = np.arange(-1, 1, 0.01) * 3                  # X-axis values: -3 to 3\n",
        "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
        "plt.title('qkv distribution')\n",
        "# A roughly bell-shaped curve centered near 0 is expected and healthy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jJM7kC4jilO"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Reshape QKV to separate attention heads\n",
        "# -------------------------------------------------------------------\n",
        "# We want to split the 1536-dim QKV into 8 heads, each handling 64 dims for Q, K, V\n",
        "#\n",
        "# num_heads = 8   → 8 parallel attention heads\n",
        "# head_dim = 512 // 8 = 64  → each head works on 64-dimensional slices\n",
        "#\n",
        "# Reshape: (1, 4, 1536) → (1, 4, 8, 192)\n",
        "#   - batch_size=1, sequence_length=4, num_heads=8, 3*head_dim=192\n",
        "#   - 192 = 3 × 64 (Q + K + V per head, each 64-dim)\n",
        "#   - This groups the QKV values by head so each head gets its own slice\n",
        "num_heads = 8\n",
        "head_dim = d_model // num_heads  # 512 / 8 = 64 dimensions per head\n",
        "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEXecxu5i9NA",
        "outputId": "b629f4d6-ec64-4f5e-937f-4c3e372f6af1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 8, 192])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify shape after reshape: should be (1, 4, 8, 192)\n",
        "# (batch=1, seq_len=4, heads=8, 3×head_dim=192)\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAqTTEP9q59y",
        "outputId": "cef8f870-b4fb-44b9-aaba-e71e4fc2801a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 192])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Permute dimensions to bring num_heads before sequence_length\n",
        "# -------------------------------------------------------------------\n",
        "# Before: (batch_size, sequence_length, num_heads, 3*head_dim) = (1, 4, 8, 192)\n",
        "# After:  (batch_size, num_heads, sequence_length, 3*head_dim) = (1, 8, 4, 192)\n",
        "#\n",
        "# WHY? We want each head to independently process ALL words in the sequence.\n",
        "# By putting num_heads in dimension 1, we can treat each head as if it were\n",
        "# a separate \"mini batch\" — matrix operations along the last two dims will\n",
        "# compute attention for each head independently and in parallel.\n",
        "#\n",
        "# Think of it as: 8 independent attention computations, each seeing all 4 words\n",
        "qkv = qkv.permute(0, 2, 1, 3)  # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJd52g7CrCqy",
        "outputId": "18f3ad8f-012d-4169-fa45-e6c8466d1795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Split the combined QKV tensor into separate Q, K, V tensors\n",
        "# -------------------------------------------------------------------\n",
        "# qkv shape: (1, 8, 4, 192)  — last dim = 192 = 3 × 64 (Q+K+V concatenated)\n",
        "#\n",
        "# .chunk(3, dim=-1) splits the last dimension into 3 equal parts:\n",
        "#   q shape: (1, 8, 4, 64)  → Query vectors: \"What am I looking for?\"\n",
        "#   k shape: (1, 8, 4, 64)  → Key vectors:   \"What do I contain?\"\n",
        "#   v shape: (1, 8, 4, 64)  → Value vectors:  \"What information do I carry?\"\n",
        "#\n",
        "# Each head now has its own independent set of 64-dim Q, K, V for all 4 words\n",
        "# Head 0 might learn to attend to syntactic structure\n",
        "# Head 1 might learn to attend to semantic meaning\n",
        "# Head 2 might learn to attend to positional proximity, etc.\n",
        "q, k, v = qkv.chunk(3, dim=-1)\n",
        "q.shape, k.shape, v.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJrxi4wdTPYO"
      },
      "source": [
        "### Summary of tensor transformations so far:\n",
        "\n",
        "| Step | Shape | Description |\n",
        "|------|-------|-------------|\n",
        "| Input `x` | (1, 4, 512) | 1 batch, 4 words, 512-dim embeddings |\n",
        "| After QKV linear | (1, 4, 1536) | Q+K+V concatenated (3 × 512) |\n",
        "| After reshape | (1, 4, 8, 192) | Split into 8 heads (3 × 64 per head) |\n",
        "| After permute | (1, 8, 4, 192) | Heads moved to dim 1 for parallel attention |\n",
        "| After chunk | 3 × (1, 8, 4, 64) | Separate Q, K, V — each head has 64-dim vectors |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUcuHtRt8H4x"
      },
      "source": [
        "## Self Attention for multiple heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5OYtIzMQ7iI"
      },
      "source": [
        "For a single head, the scaled dot-product attention formula is:\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$\n",
        "\n",
        "**Key difference from single-head attention:** Here Q, K, V are **per-head** (64-dim each), not the full 512-dim. Each of the 8 heads computes this formula independently on its own 64-dim slice. PyTorch handles all 8 heads in parallel via batch dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywffyzop0pF-",
        "outputId": "5420219e-e438-4b9d-e0df-273ba9c5915d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Compute scaled attention scores: Q · K^T / √d_k\n",
        "# -------------------------------------------------------------------\n",
        "import math\n",
        "\n",
        "# d_k = dimension of each head's key/query vector = 64\n",
        "d_k = q.size()[-1]\n",
        "\n",
        "# Matrix multiplication of Q and K^T:\n",
        "#   q shape:                (1, 8, 4, 64)\n",
        "#   k.transpose(-2, -1):   (1, 8, 64, 4)  — swap last two dims to get K^T\n",
        "#   Result (scaled):        (1, 8, 4, 4)   — attention scores for each head\n",
        "#\n",
        "# For each head: a 4×4 matrix where entry [i,j] = how much word i attends to word j\n",
        "# We divide by √d_k = √64 = 8 to stabilize the variance (prevents softmax saturation)\n",
        "#\n",
        "# Note: .transpose(-2, -1) swaps the last two dimensions.\n",
        "#   This is equivalent to transposing the (seq_len × head_dim) matrix for each head.\n",
        "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "# Result: (1, 8, 4, 4) → 1 batch, 8 heads, each with a 4×4 attention score matrix\n",
        "scaled.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIuhWR8TTGeO",
        "outputId": "3af47f70-a8c0-43f8-edb5-f2dcae5d957b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 4, 8, 1])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Why NOT use k.T for transpose? Let's see why:\n",
        "# -------------------------------------------------------------------\n",
        "# k.T transposes ALL dimensions (reverses the order of all dims):\n",
        "#   k shape:     (1, 8, 4, 64)\n",
        "#   k.T shape:   (64, 4, 8, 1)  ← This reverses ALL 4 dimensions!\n",
        "#\n",
        "# But we ONLY want to transpose the last two dims (4, 64) → (64, 4)\n",
        "# while keeping batch and head dims unchanged.\n",
        "# That's why we use k.transpose(-2, -1) which ONLY swaps dims -2 and -1:\n",
        "#   k.transpose(-2, -1) shape: (1, 8, 64, 4)  ← Correct!\n",
        "k.T.shape  # (64, 4, 8, 1) — WRONG for our purposes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkUrLeoE5Vb6",
        "outputId": "c20f9988-839a-406b-eebe-242f8da41047"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.4146,  1.1240],\n",
              "        [-0.0925,  0.0419],\n",
              "        [-1.1873,  0.4655]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Quick demo: torch.transpose() swaps exactly TWO specified dimensions\n",
        "# -------------------------------------------------------------------\n",
        "# For a 2D tensor, transpose(0,1) and transpose(1,0) are identical — \n",
        "# both swap rows and columns (like a regular matrix transpose)\n",
        "y = torch.randn(2, 3)            # 2×3 matrix\n",
        "torch.transpose(y, 0, 1)          # → 3×2 matrix (swap dim 0 and dim 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMaODlo-5Ygz",
        "outputId": "bc61ffd0-9db4-4d4a-dc01-7ee9a3456a3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.4146,  1.1240],\n",
              "        [-0.0925,  0.0419],\n",
              "        [-1.1873,  0.4655]])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Same result — order of arguments doesn't matter for 2D tensors\n",
        "# transpose(1, 0) == transpose(0, 1) for 2D tensors\n",
        "torch.transpose(y, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PL2TSC4ZTc",
        "outputId": "403fdc2a-8523-4f50-c4a2-38fd3a7af462"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          ...,\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True]],\n",
              "\n",
              "         [[True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          ...,\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True]],\n",
              "\n",
              "         [[True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          ...,\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          ...,\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True]],\n",
              "\n",
              "         [[True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          ...,\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True]],\n",
              "\n",
              "         [[True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          ...,\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True],\n",
              "          [True, True, True, True]]]])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Verify: transpose(-1, -2) == transpose(-2, -1) for our K tensor\n",
        "# -------------------------------------------------------------------\n",
        "# Negative indexing: -1 = last dim, -2 = second-to-last dim\n",
        "# Both swap the same pair of dimensions, so the result is identical\n",
        "# This confirms that the order of the two dimension arguments doesn't matter\n",
        "k.transpose(-1, -2) == k.transpose(-2, -1)  # All True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daYW7MtI49t8",
        "outputId": "42d4c383-096d-49e3-a605-3d68d0274385"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 64, 4])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Correct transposed K shape: (1, 8, 64, 4)\n",
        "# Batch(1) and heads(8) are untouched; only seq_len(4) and head_dim(64) are swapped\n",
        "k.transpose(-1, -2).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6tN6jvA0qur",
        "outputId": "84f0b8f6-0b97-431a-a0df-7c70ec973d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Create the causal (look-ahead) mask for the DECODER\n",
        "# -------------------------------------------------------------------\n",
        "# Purpose: Prevent each word from attending to FUTURE words during decoding.\n",
        "#\n",
        "# Step 1: Create a matrix filled with -infinity, same size as the attention scores\n",
        "#   torch.full(scaled.size(), float('-inf')) → shape (1, 8, 4, 4), all values = -inf\n",
        "#\n",
        "# Step 2: torch.triu(mask, diagonal=1) keeps only the UPPER triangle (above the diagonal)\n",
        "#   - diagonal=1 means: keep elements ABOVE the main diagonal (exclude diagonal itself)\n",
        "#   - Elements on and below the diagonal become 0\n",
        "#\n",
        "# Result for each head's 4×4 mask:\n",
        "#   [[  0, -inf, -inf, -inf],    ← word 0: can only see itself\n",
        "#    [  0,    0, -inf, -inf],    ← word 1: can see words 0-1\n",
        "#    [  0,    0,    0, -inf],    ← word 2: can see words 0-2\n",
        "#    [  0,    0,    0,    0]]    ← word 3: can see all words 0-3\n",
        "#\n",
        "# When added to attention scores:\n",
        "#   0 entries → score unchanged (allowed to attend)\n",
        "#   -inf entries → score becomes -inf → softmax(-inf) = 0 (blocked)\n",
        "mask = torch.full(scaled.size() , float('-inf'))\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "# Display the mask for head 1 (index 1) — all heads share the same mask pattern\n",
        "mask[0][1]  # mask for input to a single head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ8nYfdm1vis",
        "outputId": "7dc6378f-c50b-41d0-bece-ac2ff837f579"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0611,    -inf,    -inf,    -inf],\n",
              "        [ 0.5596,  0.0404,    -inf,    -inf],\n",
              "        [ 0.1348, -0.0516,  0.3945,    -inf],\n",
              "        [ 0.0038,  0.5560, -0.2397,  0.4025]], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Preview: what do the masked attention scores look like for head 0, word 0?\n",
        "# -------------------------------------------------------------------\n",
        "# (scaled + mask)[0][0] = batch 0, head 0\n",
        "# For word 0 (first row): only [0,0] has a real score, rest are -inf\n",
        "# This ensures word 0 can ONLY attend to itself during decoding\n",
        "(scaled + mask)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efXdwo2U3FyI"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Apply the mask to the scaled attention scores (in-place addition)\n",
        "# -------------------------------------------------------------------\n",
        "# scaled shape: (1, 8, 4, 4) — attention scores for each head\n",
        "# mask shape:   (1, 8, 4, 4) — 0 for allowed, -inf for blocked\n",
        "#\n",
        "# After this: future positions have -inf scores → softmax will zero them out\n",
        "# Note: += modifies scaled in-place (no new tensor created)\n",
        "scaled += mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTazQyVQ3tz1",
        "outputId": "9c9c4ccc-e589-445a-810c-fef9c1339071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6269606805367254"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Quick softmax sanity check with actual numbers\n",
        "# -------------------------------------------------------------------\n",
        "# If we have two scores: 0.5596 and 0.0404, softmax should give:\n",
        "#   softmax(0.5596) = e^0.5596 / (e^0.5596 + e^0.0404)\n",
        "#\n",
        "# This verifies our understanding: the higher score (0.5596) gets a LARGER\n",
        "# probability weight. Softmax converts raw scores into a probability distribution\n",
        "# where all values are between 0 and 1, and they sum to 1.\n",
        "np.exp(0.5596) / (np.exp(0.5596) + np.exp(0.0404))  # Should be ~0.63"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9aR2BNP3Gw_"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Apply softmax to get attention weights (probability distributions)\n",
        "# -------------------------------------------------------------------\n",
        "# F.softmax(scaled, dim=-1):\n",
        "#   - Applies softmax along the LAST dimension (dim=-1 = across columns)\n",
        "#   - Each row becomes a probability distribution summing to 1.0\n",
        "#   - Entries that were -inf become 0 probability (masked future words!)\n",
        "#\n",
        "# Input:  scaled shape (1, 8, 4, 4) — raw masked scores\n",
        "# Output: attention shape (1, 8, 4, 4) — probability weights\n",
        "#\n",
        "# For each head, each word (row) now has a probability distribution over\n",
        "# all words it's allowed to attend to.\n",
        "# Example row for word 2: [0.15, 0.25, 0.60, 0.00]\n",
        "#   → 15% attention to word 0, 25% to word 1, 60% to itself, 0% to word 3 (masked)\n",
        "attention = F.softmax(scaled, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kKHJqT83JSy",
        "outputId": "9cefdac2-080d-444c-8050-8643c2c0163c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Attention shape: (1, 8, 4, 4)\n",
        "# (batch=1, heads=8, seq_len=4, seq_len=4)\n",
        "# Each head has its own 4×4 attention weight matrix\n",
        "attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SImzTnAl3L21",
        "outputId": "368b64c2-d53b-4dba-de02-d6a363e9d9af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.6270, 0.3730, 0.0000, 0.0000],\n",
              "        [0.3198, 0.2655, 0.4147, 0.0000],\n",
              "        [0.1996, 0.3467, 0.1564, 0.2973]], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# View attention weights for batch 0, head 0\n",
        "# -------------------------------------------------------------------\n",
        "# This 4×4 matrix shows how head 0 distributes attention:\n",
        "#   Row 0: word 0's attention → [prob, 0, 0, 0]     (only sees itself)\n",
        "#   Row 1: word 1's attention → [prob, prob, 0, 0]   (sees words 0-1)\n",
        "#   Row 2: word 2's attention → [prob, prob, prob, 0] (sees words 0-2)\n",
        "#   Row 3: word 3's attention → [prob, prob, prob, prob] (sees all words)\n",
        "#\n",
        "# Notice: upper triangle is 0 (masked!) and each row sums to 1.0\n",
        "attention[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNB5BsNyRYZP",
        "outputId": "4eeb673c-3f44-4f74-c2da-53a0321980b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Compute new context-aware Value vectors: attention · V\n",
        "# -------------------------------------------------------------------\n",
        "# Matrix multiplication: attention weights × value vectors\n",
        "#   attention shape: (1, 8, 4, 4)  — how much each word attends to others\n",
        "#   v shape:         (1, 8, 4, 64) — original value vectors per head\n",
        "#   Result:          (1, 8, 4, 64) — new context-enriched value vectors per head\n",
        "#\n",
        "# For each head and each word i:\n",
        "#   values[i] = Σ_j attention[i][j] × v[j]\n",
        "#   = weighted sum of ALL value vectors, using attention probabilities as weights\n",
        "#\n",
        "# This is the KEY step: each word's output now contains information from\n",
        "# the words it attended to, weighted by how relevant they are!\n",
        "values = torch.matmul(attention, v)\n",
        "values.shape  # (1, 8, 4, 64) — 8 heads, each producing 64-dim output per word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAp7B9gDRgvW"
      },
      "source": [
        "## Function\n",
        "\n",
        "Encapsulate the scaled dot-product attention into a reusable function.\n",
        "This function handles both **encoder** (no mask → full bidirectional attention) and **decoder** (with mask → causal attention) cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C0rAup-rWNo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Compute Scaled Dot-Product Attention (works for all heads in parallel).\n",
        "    \n",
        "    Formula: Attention(Q, K, V) = softmax( Q·K^T / √d_k + mask ) · V\n",
        "    \n",
        "    Args:\n",
        "        q:    Query tensor  — shape (batch, num_heads, seq_len, head_dim)\n",
        "        k:    Key tensor    — shape (batch, num_heads, seq_len, head_dim)\n",
        "        v:    Value tensor  — shape (batch, num_heads, seq_len, head_dim)\n",
        "        mask: Optional causal mask — shape (batch, num_heads, seq_len, seq_len)\n",
        "              - 0 for allowed positions, -inf for blocked (future) positions\n",
        "              - Pass None for encoder (no masking needed)\n",
        "              - Pass triangular mask for decoder (prevent future peeking)\n",
        "    \n",
        "    Returns:\n",
        "        values:    Context-enriched output — shape (batch, num_heads, seq_len, head_dim)\n",
        "        attention: Attention weights       — shape (batch, num_heads, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # Step 1: Get d_k from the last dimension of Q\n",
        "    d_k = q.size()[-1]  # head_dim (e.g., 64)\n",
        "    \n",
        "    # Step 2: Compute Q·K^T / √d_k → scaled attention scores\n",
        "    # q @ k^T → (batch, heads, seq, seq) — pairwise attention scores\n",
        "    # Dividing by √d_k stabilizes gradients (prevents softmax saturation)\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    \n",
        "    # Step 3: Apply mask (only for decoder — blocks future token attention)\n",
        "    if mask is not None:\n",
        "        scaled += mask  # -inf scores → 0 probability after softmax\n",
        "    \n",
        "    # Step 4: Softmax → convert scores to probability distribution per row\n",
        "    attention = F.softmax(scaled, dim=-1)  # Each row sums to 1.0\n",
        "    \n",
        "    # Step 5: Weighted sum of value vectors using attention weights\n",
        "    values = torch.matmul(attention, v)  # (batch, heads, seq, head_dim)\n",
        "    \n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plrxn94Irs2K"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Run the attention function with our Q, K, V and mask\n",
        "# -------------------------------------------------------------------\n",
        "# This computes attention for all 8 heads simultaneously in one call!\n",
        "values, attention = scaled_dot_product(q, k, v, mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4agepAfr8_u",
        "outputId": "30ad86c7-bfd4-4d61-d645-5a1c00d48779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Attention shape: (1, 8, 4, 4)\n",
        "# Each of 8 heads has its own 4×4 attention weight matrix\n",
        "attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts-vwtNXrjFP",
        "outputId": "b493b5fc-b715-48fb-80d7-61b1a4dd2599"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.6270, 0.3730, 0.0000, 0.0000],\n",
              "        [0.3198, 0.2655, 0.4147, 0.0000],\n",
              "        [0.1996, 0.3467, 0.1564, 0.2973]], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View attention weights for batch 0, head 0\n",
        "# Lower triangle has probabilities, upper triangle is 0 (masked)\n",
        "# Each row sums to 1.0\n",
        "attention[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzMrh7Q8sKW5",
        "outputId": "a43e6c78-c9bd-47e2-eafe-1b91e4453362"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Values shape: (1, 8, 4, 64)\n",
        "# 8 heads × 4 words × 64-dim output per head\n",
        "# These are the new context-aware representations — but still separated by head\n",
        "values.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqkGc4zdsOQ3",
        "outputId": "6dd47f8d-6a6c-4473-d810-9441155bd65b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Concatenate all heads back into a single vector per word\n",
        "# -------------------------------------------------------------------\n",
        "# Current shape: (1, 8, 4, 64) → (batch, heads, seq_len, head_dim)\n",
        "# Target shape:  (1, 4, 512)   → (batch, seq_len, d_model)\n",
        "#\n",
        "# Reshape merges num_heads × head_dim = 8 × 64 = 512 back into one dimension\n",
        "# This concatenates the outputs from all 8 heads for each word\n",
        "# Now each word has a single 512-dim vector that captures ALL heads' perspectives\n",
        "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n",
        "values.size()  # (1, 4, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7DH6VKMtMTu"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Final linear layer: mix information ACROSS attention heads\n",
        "# -------------------------------------------------------------------\n",
        "# nn.Linear(d_model, d_model) = nn.Linear(512, 512)\n",
        "#\n",
        "# WHY do we need this?\n",
        "# After concatenation, each 64-dim chunk only contains info from ONE head.\n",
        "# The heads haven't \"talked\" to each other yet!\n",
        "# This linear layer applies a learned weight matrix W_O (512 × 512) that\n",
        "# allows the model to combine information from ALL heads into a final output.\n",
        "#\n",
        "# This is the W_O matrix from the paper: MultiHead(Q,K,V) = Concat(head_1,...,head_h) × W_O\n",
        "linear_layer = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6zt1i51thgO"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Pass concatenated values through the linear layer\n",
        "# -------------------------------------------------------------------\n",
        "# Input:  (1, 4, 512) — concatenated head outputs\n",
        "# Output: (1, 4, 512) — final multi-head attention output\n",
        "# Each word now has a 512-dim vector that encodes:\n",
        "#   ✓ Context from all other words (via attention)\n",
        "#   ✓ Multiple perspectives (via 8 different heads)\n",
        "#   ✓ Cross-head information mixing (via this linear layer)\n",
        "out = linear_layer(values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38nDP4hGtjol",
        "outputId": "e1f02961-d7d2-4c2b-d496-9325258cdbc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Final output shape: (1, 4, 512)\n",
        "# Same shape as the input! This is by design — multi-head attention\n",
        "# is a \"context enrichment\" operation that preserves dimensionality\n",
        "# so it can be stacked in multiple layers\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2NIAo2X6gIZ",
        "outputId": "6636d212-208d-4138-be57-81965c3a2d39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.0687,  0.0984,  0.0381,  ...,  0.1252,  0.0707, -0.0547],\n",
              "         [-0.0399,  0.0419, -0.0933,  ...,  0.3064,  0.1585,  0.0599],\n",
              "         [-0.0684, -0.0657,  0.2306,  ...,  0.0606,  0.0670,  0.0293],\n",
              "         [ 0.0781, -0.0159, -0.0343,  ...,  0.0074, -0.1094, -0.0365]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the actual output values\n",
        "# Each of the 4 words now has a 512-dim context-enriched representation\n",
        "# These would next be passed to Add & Norm → Feed Forward → etc. in the Transformer\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmRfc7fhtc1U"
      },
      "source": [
        "## Class\n",
        "\n",
        "Encapsulate the **entire multi-head attention mechanism** into a reusable `nn.Module` class. This bundles together:\n",
        "1. The QKV projection (linear layer)\n",
        "2. Reshaping into multiple heads\n",
        "3. Scaled dot-product attention (per head)\n",
        "4. Concatenation of heads\n",
        "5. Output linear projection (cross-head mixing)\n",
        "\n",
        "This class can be used as a drop-in component in both the **encoder** and **decoder** of a Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSIKbDEXtcOv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Compute Scaled Dot-Product Attention for all heads in parallel.\n",
        "    (Same function as defined above, included here for self-contained class)\n",
        "    \"\"\"\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention module for Transformer neural networks.\n",
        "    \n",
        "    This implements the full multi-head attention mechanism:\n",
        "        MultiHead(Q, K, V) = Concat(head_1, ..., head_h) · W_O\n",
        "    \n",
        "    where each head_i = Attention(Q·W_Q_i, K·W_K_i, V·W_V_i)\n",
        "    \n",
        "    Architecture:\n",
        "        Input (batch, seq_len, input_dim)\n",
        "          → QKV Linear Layer → (batch, seq_len, 3 * d_model)\n",
        "          → Reshape & Split into heads → Q, K, V each (batch, num_heads, seq_len, head_dim)\n",
        "          → Scaled Dot-Product Attention per head → (batch, num_heads, seq_len, head_dim)\n",
        "          → Concatenate heads → (batch, seq_len, d_model)\n",
        "          → Output Linear Layer → (batch, seq_len, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim:  Dimension of input word embeddings (e.g., 512 or 1024)\n",
        "            d_model:    Internal model dimension (typically 512)\n",
        "            num_heads:  Number of parallel attention heads (typically 8)\n",
        "                        d_model must be divisible by num_heads\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads  # e.g., 512 // 8 = 64 per head\n",
        "        \n",
        "        # Single linear layer to produce Q, K, V all at once (more efficient!)\n",
        "        # Maps: input_dim → 3 * d_model (e.g., 512 → 1536)\n",
        "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
        "        \n",
        "        # Output linear layer: mixes information across all attention heads\n",
        "        # Maps: d_model → d_model (e.g., 512 → 512)\n",
        "        # This is the W_O matrix from the \"Attention Is All You Need\" paper\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of Multi-Head Attention.\n",
        "        \n",
        "        Args:\n",
        "            x:    Input tensor — shape (batch_size, sequence_length, input_dim)\n",
        "            mask: Optional causal mask — shape matching (batch, heads, seq, seq)\n",
        "        \n",
        "        Returns:\n",
        "            out:  Context-enriched output — shape (batch_size, sequence_length, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        # x: (batch, seq_len, input_dim) e.g., (30, 5, 1024)\n",
        "        \n",
        "        # ---- Step 1: Project input to Q, K, V ----\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        # qkv: (batch, seq_len, 3*d_model) e.g., (30, 5, 1536)\n",
        "        \n",
        "        # ---- Step 2: Reshape to separate heads ----\n",
        "        # Split the 3*d_model dimension into num_heads groups of 3*head_dim\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        # qkv: (batch, seq_len, num_heads, 3*head_dim) e.g., (30, 5, 8, 192)\n",
        "        \n",
        "        # ---- Step 3: Move heads dimension before sequence ----\n",
        "        # So each head can independently process the full sequence\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        # qkv: (batch, num_heads, seq_len, 3*head_dim) e.g., (30, 8, 5, 192)\n",
        "        \n",
        "        # ---- Step 4: Split into separate Q, K, V tensors ----\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        # q, k, v: each (batch, num_heads, seq_len, head_dim) e.g., (30, 8, 5, 64)\n",
        "        \n",
        "        # ---- Step 5: Compute scaled dot-product attention for ALL heads ----\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        # values: (batch, num_heads, seq_len, head_dim) e.g., (30, 8, 5, 64)\n",
        "        # attention: (batch, num_heads, seq_len, seq_len) e.g., (30, 8, 5, 5)\n",
        "        \n",
        "        # ---- Step 6: Concatenate all heads back together ----\n",
        "        # Merge num_heads × head_dim = 8 × 64 = 512 back into d_model\n",
        "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        # values: (batch, seq_len, d_model) e.g., (30, 5, 512)\n",
        "        \n",
        "        # ---- Step 7: Final linear projection (W_O) ----\n",
        "        # This allows cross-head communication and produces the final output\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        # out: (batch, seq_len, d_model) e.g., (30, 5, 512)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ux6hMEjyWEU"
      },
      "source": [
        "## Input & Testing\n",
        "\n",
        "Test the `MultiheadAttention` class with a larger, more realistic input to verify it handles different batch sizes, sequence lengths, and input dimensions correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiB-SkfaxCTl",
        "outputId": "04f0a850-3a4d-427d-9345-f17082789c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.size(): torch.Size([30, 5, 1024])\n",
            "qkv.size(): torch.Size([30, 5, 1536])\n",
            "qkv.size(): torch.Size([30, 5, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 5, 192])\n",
            "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
            "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
            "values.size(): torch.Size([30, 5, 512])\n",
            "out.size(): torch.Size([30, 5, 512])\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Test the MultiheadAttention class with realistic parameters\n",
        "# -------------------------------------------------------------------\n",
        "# input_dim = 1024       → Input embeddings have 1024 dimensions\n",
        "#                          (This can differ from d_model; the QKV layer handles the projection)\n",
        "# d_model = 512          → Internal Transformer dimension (from \"Attention Is All You Need\")\n",
        "# num_heads = 8          → 8 parallel attention heads (512 / 8 = 64 dims per head)\n",
        "#\n",
        "# batch_size = 30        → Process 30 sentences simultaneously (realistic training batch)\n",
        "# sequence_length = 5    → Each sentence has 5 tokens\n",
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "batch_size = 30\n",
        "sequence_length = 5\n",
        "\n",
        "# Create random input: 30 sentences × 5 words × 1024-dim embeddings\n",
        "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
        "\n",
        "# Instantiate the MultiheadAttention module\n",
        "# This creates all the learned weight matrices (QKV projection + output projection)\n",
        "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
        "\n",
        "# Forward pass: transforms (30, 5, 1024) → (30, 5, 512)\n",
        "# Watch the print statements to trace the tensor shape through each step:\n",
        "#   x:         (30, 5, 1024) → input\n",
        "#   qkv:       (30, 5, 1536) → after QKV linear layer\n",
        "#   qkv:       (30, 5, 8, 192) → after reshape into heads\n",
        "#   qkv:       (30, 8, 5, 192) → after permute (heads before seq)\n",
        "#   q, k, v:   (30, 8, 5, 64) → after splitting into Q, K, V\n",
        "#   values:    (30, 8, 5, 64) → after attention computation\n",
        "#   values:    (30, 5, 512)   → after concatenating heads (8 × 64 = 512)\n",
        "#   out:       (30, 5, 512)   → after final linear layer\n",
        "out = model.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84AaNS24xuUV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
