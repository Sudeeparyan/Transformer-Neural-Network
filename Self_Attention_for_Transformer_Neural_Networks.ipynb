{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO5Z0qErNuOt"
      },
      "source": [
        "# Self Attention in Transformers\n",
        "\n",
        "**Self-attention** is the core mechanism inside Transformer neural networks.  \n",
        "It allows every word in a sentence to look at **every other word** (including itself) to build a richer, context-aware representation.\n",
        "\n",
        "Unlike RNNs which process words sequentially and struggle with long-range dependencies, self-attention processes all words **in parallel** and captures **both past and future context** simultaneously.\n",
        "\n",
        "### How it works at a high level:\n",
        "1. Each word generates three vectors: **Query (Q)**, **Key (K)**, and **Value (V)**.\n",
        "2. The **dot product** of Q and K determines how much attention each word pays to every other word.\n",
        "3. These scores are **scaled** (divided by √d_k) to keep gradients stable.\n",
        "4. An optional **mask** is applied (in the decoder) to prevent attending to future words.\n",
        "5. **Softmax** converts raw scores into a probability distribution.\n",
        "6. The probability-weighted sum of **V** vectors produces the new context-aware representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HedntyUvLrBo"
      },
      "source": [
        "## Generate Data\n",
        "\n",
        "We create random **Query (Q)**, **Key (K)**, and **Value (V)** matrices to simulate the inputs to self-attention.\n",
        "\n",
        "- **L** (sequence length) = 4 → represents 4 words/tokens in a sentence (e.g., \"my name is Aj\").\n",
        "- **d_k** (key/query dimension) = 8 → each Q and K vector has 8 dimensions.\n",
        "- **d_v** (value dimension) = 8 → each V vector has 8 dimensions.\n",
        "\n",
        "In a real Transformer, Q, K, V are produced by multiplying the input embeddings (typically 512-dim) with learned weight matrices. Here we use random values for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtKbaWhFJui3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Define dimensions for our self-attention example\n",
        "# -------------------------------------------------------------------\n",
        "# L   = Sequence length (number of tokens/words in the input sentence)\n",
        "#       Example: \"my name is Aj\" → 4 tokens, so L = 4\n",
        "# d_k = Dimension of the Query (Q) and Key (K) vectors\n",
        "#       Each word's Q and K will be a vector of size 8\n",
        "# d_v = Dimension of the Value (V) vectors\n",
        "#       Each word's V will be a vector of size 8\n",
        "# In real Transformers, d_k and d_v are typically 64 (with 512-dim embeddings split across 8 heads)\n",
        "L, d_k, d_v = 4, 8, 8\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Generate random Q, K, V matrices\n",
        "# -------------------------------------------------------------------\n",
        "# q (Query):  Shape (L, d_k) = (4, 8)\n",
        "#   - \"What am I looking for?\" — each word asks what context it needs\n",
        "# k (Key):    Shape (L, d_k) = (4, 8)\n",
        "#   - \"What can I offer?\" — each word advertises what information it contains\n",
        "# v (Value):  Shape (L, d_v) = (4, 8)\n",
        "#   - \"What information do I actually carry?\" — the real content each word provides\n",
        "#\n",
        "# np.random.randn generates values from a standard normal distribution (mean=0, std=1)\n",
        "q = np.random.randn(L, d_k)   # 4 words, each with an 8-dim query vector\n",
        "k = np.random.randn(L, d_k)   # 4 words, each with an 8-dim key vector\n",
        "v = np.random.randn(L, d_v)   # 4 words, each with an 8-dim value vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09JpvuNJ2sZC",
        "outputId": "30d2c627-8647-44e0-aa92-c9e53e3b7843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[ 0.11672673 -2.54870451 -1.44065948  0.93661829  1.36278968  1.04252277\n",
            "  -0.01310938 -1.3163937 ]\n",
            " [ 0.26721599 -0.90218255  0.07417847 -0.10430246  0.52684253 -0.07081531\n",
            "  -0.60511725 -0.55225527]\n",
            " [-0.93297509  0.28724456  1.37184579  0.41589874  0.34981245 -0.24753755\n",
            "  -1.24497125  0.05044148]\n",
            " [-0.11414585 -0.01545749 -0.58376828 -0.40193907  0.93931836 -1.94334363\n",
            "  -0.34770465  1.50103406]]\n",
            "K\n",
            " [[ 1.1226585  -0.85645535  0.54315044  1.36560451  0.52539476 -0.94502504\n",
            "  -0.48444661  0.46268014]\n",
            " [-0.53713766 -1.16937329 -0.57988617  0.92713577 -0.85995607 -0.40352635\n",
            "   0.26555146 -1.83159914]\n",
            " [-2.06994435 -0.09514715 -1.64928361 -0.17375184  0.13146819 -1.76335363\n",
            "   1.56568846  0.69751826]\n",
            " [ 0.32910684 -0.1939204  -0.80444134  0.78816869  0.35599408  0.28309835\n",
            "  -0.25970963  1.49744622]]\n",
            "V\n",
            " [[-0.00368231  1.43739233 -0.59614565 -1.23171219  1.12030717 -0.98620738\n",
            "  -0.15461465 -1.03106383]\n",
            " [ 0.85585446 -1.79878344  0.67321704  0.05607552 -0.15542661 -1.41264124\n",
            "  -0.40136933 -1.17626611]\n",
            " [ 0.50465335  2.28693419  0.67128338  0.2506863   1.78802234  0.14775751\n",
            "  -0.11405725  0.88026286]\n",
            " [-0.68069105  0.68385101  0.17994557 -1.68013201  0.91543969 -0.19108312\n",
            "   0.03160471  1.40527326]]\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Print the generated Q, K, V matrices\n",
        "# -------------------------------------------------------------------\n",
        "# Each matrix is (4, 8): 4 rows (one per word) × 8 columns (vector dimensions)\n",
        "# Row 0 → word 1 (\"my\"), Row 1 → word 2 (\"name\"), etc.\n",
        "print(\"Q\\n\", q)   # Query matrix — what each word is searching for\n",
        "print(\"K\\n\", k)   # Key matrix — what each word offers as a match\n",
        "print(\"V\\n\", v)   # Value matrix — the actual information each word carries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV6txskBLwjh"
      },
      "source": [
        "## Self Attention\n",
        "\n",
        "The self-attention formula computes how much each word should attend to every other word:\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "\n",
        "**Step-by-step breakdown:**\n",
        "1. **Q · K^T** → Dot product of Query and Key (transposed). Produces an (L × L) matrix where entry [i, j] = how much word i should attend to word j.\n",
        "2. **÷ √d_k** → Scale down to reduce variance (prevents softmax from producing extreme values / vanishing gradients).\n",
        "3. **+ M** → Add mask (optional, used in decoder to block future words).\n",
        "4. **softmax** → Convert raw scores into probabilities (each row sums to 1).\n",
        "\n",
        "Then compute the new context-aware value vectors:\n",
        "\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$\n",
        "\n",
        "Each word's new representation = weighted sum of **all** value vectors, where the weights come from the attention scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7GePHKk3Mh0",
        "outputId": "7dae7f5e-4715-4fd4-fbfd-7c0815e7d39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.9385252 ,  5.43647918, -0.38370563,  1.24225801],\n",
              "       [ 1.35187753,  1.19807371, -1.70999851, -0.38129862],\n",
              "       [ 1.06382646, -0.86860778, -1.86251774, -0.68520405],\n",
              "       [ 2.21209236, -2.81995366,  5.32327746,  2.24049732]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 1: Compute Q · K^T  (raw attention scores, BEFORE scaling)\n",
        "# -------------------------------------------------------------------\n",
        "# np.matmul(q, k.T) performs matrix multiplication:\n",
        "#   q shape:   (L, d_k) = (4, 8)\n",
        "#   k.T shape: (d_k, L) = (8, 4)\n",
        "#   Result:    (L, L)   = (4, 4)\n",
        "#\n",
        "# The resulting 4×4 matrix:\n",
        "#   - Row i, Column j = dot product of word i's Query with word j's Key\n",
        "#   - Higher value → word i should pay MORE attention to word j\n",
        "#   - This is the \"affinity\" or \"compatibility\" between every pair of words\n",
        "#\n",
        "# Example: entry [0, 2] tells us how much word 0 (\"my\") attends to word 2 (\"is\")\n",
        "np.matmul(q, k.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odK76OoI3nL2",
        "outputId": "69b50cdb-9a41-45ae-bfd2-619228af1ef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8672192297664698, 0.9229851723027697, 5.1446872979260165)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# WHY do we need to divide by √d_k?  (Variance analysis)\n",
        "# -------------------------------------------------------------------\n",
        "# When Q and K are drawn from standard normal distributions (mean=0, var=1),\n",
        "# their dot product has variance ≈ d_k (the dimension of the vectors).\n",
        "#\n",
        "# Problem: If d_k is large (e.g., 64 or 512), the dot product values become\n",
        "# very large, pushing softmax into regions with extremely small gradients\n",
        "# (the \"saturation\" zone), making training very slow or unstable.\n",
        "#\n",
        "# Solution: Divide by √d_k to bring the variance back to approximately 1.\n",
        "#\n",
        "# Let's verify:\n",
        "#   - q.var() ≈ 1       (Q values have variance ~1, as expected from randn)\n",
        "#   - k.var() ≈ 1       (K values have variance ~1)\n",
        "#   - (Q·K^T).var() ≈ d_k = 8  (unscaled dot product has inflated variance!)\n",
        "q.var(), k.var(), np.matmul(q, k.T).var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ps6AY1Q3tRI",
        "outputId": "3b9ac3c8-70b8-47bd-e868-e7d6fd26d270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8672192297664698, 0.9229851723027697, 0.643085912240752)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 2: Scale the dot product by dividing by √d_k\n",
        "# -------------------------------------------------------------------\n",
        "# math.sqrt(d_k) = √8 ≈ 2.83\n",
        "# Dividing by this factor reduces the variance of the scores from ~d_k back to ~1\n",
        "# This ensures softmax receives moderate values, producing well-distributed\n",
        "# attention weights (not all near 0 or 1).\n",
        "#\n",
        "# scaled shape: (4, 4) — same as before, but values are smaller and more stable\n",
        "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "\n",
        "# Verify the variance reduction:\n",
        "#   - q.var() ≈ 1 (unchanged)\n",
        "#   - k.var() ≈ 1 (unchanged)\n",
        "#   - scaled.var() ≈ 1  ← SUCCESS! Variance is now close to 1 instead of ~8\n",
        "q.var(), k.var(), scaled.var()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypO9IK1PL3cJ"
      },
      "source": [
        "Notice the reduction in variance of the product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVHAJR4N4VQX",
        "outputId": "52b06cf8-0381-453c-b576-0bd8de9a38b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.68537216,  1.92208565, -0.13566043,  0.43920453],\n",
              "       [ 0.47796088,  0.42358302, -0.60457577, -0.13480942],\n",
              "       [ 0.37611945, -0.30709922, -0.65849946, -0.24225621],\n",
              "       [ 0.78209275, -0.99700418,  1.88206279,  0.79213542]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Display the scaled attention scores matrix\n",
        "# -------------------------------------------------------------------\n",
        "# This is a 4×4 matrix (L × L):\n",
        "#   - Each row corresponds to a word in the sentence\n",
        "#   - Each column corresponds to a word it could attend to\n",
        "#   - Values are now in a reasonable range (roughly -2 to +2) thanks to scaling\n",
        "#   - These scores will next be masked (in decoder) and then passed through softmax\n",
        "scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmz4v-RmMAaj"
      },
      "source": [
        "## Masking\n",
        "\n",
        "- **Purpose:** Prevent words from \"cheating\" by looking at future words that haven't been generated yet.\n",
        "- **Where it's used:** Only in the **decoder** (not in the encoder).\n",
        "  - **Encoder:** Every word can attend to every other word (full context) → no mask needed.\n",
        "  - **Decoder:** During generation, word at position t can only attend to words at positions ≤ t → mask required.\n",
        "- **How it works:**\n",
        "  - A lower-triangular matrix of 1s is created (future positions = 0).\n",
        "  - Zeros are replaced with **-∞** (negative infinity) so that after softmax, those positions get a probability of 0.\n",
        "  - Ones are replaced with **0** so they don't change the attention scores when added.\n",
        "\n",
        "**Example for L=4:**\n",
        "```\n",
        "Word 1 can see: [word1,   -∞,    -∞,    -∞  ]\n",
        "Word 2 can see: [word1, word2,   -∞,    -∞  ]\n",
        "Word 3 can see: [word1, word2, word3,   -∞  ]\n",
        "Word 4 can see: [word1, word2, word3, word4 ]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8N3OhSLILfG",
        "outputId": "2c63a444-066c-44b2-abe5-242dd989f311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 3a: Create the causal (look-ahead) mask\n",
        "# -------------------------------------------------------------------\n",
        "# np.tril = lower TRIangular matrix (keeps values on and below the diagonal)\n",
        "# np.ones((L, L)) creates a 4×4 matrix of all 1s, then tril zeros out the upper triangle\n",
        "#\n",
        "# Result:\n",
        "#   [[1, 0, 0, 0],    ← word 0 can only see itself\n",
        "#    [1, 1, 0, 0],    ← word 1 can see words 0 and 1\n",
        "#    [1, 1, 1, 0],    ← word 2 can see words 0, 1, and 2\n",
        "#    [1, 1, 1, 1]]    ← word 3 can see all words (0, 1, 2, 3)\n",
        "#\n",
        "# 1 = \"allowed to attend\" | 0 = \"blocked (future word)\"\n",
        "mask = np.tril(np.ones( (L, L) ))\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIV9K3Yn6s1V"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 3b: Convert the binary mask into an additive mask\n",
        "# -------------------------------------------------------------------\n",
        "# We transform 0s → -infinity and 1s → 0 so we can ADD this mask to the scores:\n",
        "#\n",
        "#   mask[mask == 0] = -inf   → blocked positions become -∞\n",
        "#       When we later add -∞ to an attention score, softmax(−∞) → 0\n",
        "#       This means the model assigns ZERO attention to future words.\n",
        "#\n",
        "#   mask[mask == 1] = 0      → allowed positions become 0\n",
        "#       Adding 0 leaves the attention score unchanged.\n",
        "#\n",
        "# After this transformation, the mask looks like:\n",
        "#   [[  0, -inf, -inf, -inf],\n",
        "#    [  0,    0, -inf, -inf],\n",
        "#    [  0,    0,    0, -inf],\n",
        "#    [  0,    0,    0,    0]]\n",
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK5V_T3W6vpX",
        "outputId": "bb4160a1-a011-4850-e403-9cb252572c66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Display the additive mask\n",
        "# -------------------------------------------------------------------\n",
        "# Verify the mask:\n",
        "#   - 0 entries → these positions are allowed (attention score unchanged)\n",
        "#   - -inf entries → these positions are blocked (softmax will output 0 here)\n",
        "# Row i shows which words word i is allowed to see (0) vs blocked from seeing (-inf)\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNH1VgEf7xTa",
        "outputId": "4211c411-0356-4e39-8388-d39b0c1d0920"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.68537216,        -inf,        -inf,        -inf],\n",
              "       [ 0.47796088,  0.42358302,        -inf,        -inf],\n",
              "       [ 0.37611945, -0.30709922, -0.65849946,        -inf],\n",
              "       [ 0.78209275, -0.99700418,  1.88206279,  0.79213542]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 3c: Apply the mask to the scaled attention scores\n",
        "# -------------------------------------------------------------------\n",
        "# Element-wise addition: scaled + mask\n",
        "#   - Where mask is 0 → score stays the same (word is visible)\n",
        "#   - Where mask is -inf → score becomes -inf (word is hidden/future)\n",
        "#\n",
        "# After softmax, -inf entries will become 0 probability, effectively\n",
        "# preventing the decoder from \"peeking\" at future tokens.\n",
        "#\n",
        "# Result shape: (4, 4) — same as the scaled matrix\n",
        "scaled + mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMTAXjooN9eZ"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "Softmax converts raw attention scores into a **probability distribution** — each row sums to 1.\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
        "$$\n",
        "\n",
        "**Why softmax?**\n",
        "- It turns arbitrary real-valued scores into values between 0 and 1.\n",
        "- Higher scores → higher probability → more attention.\n",
        "- Scores of **-∞** (masked future words) → e^(-∞) = 0 → zero attention (exactly what we want!).\n",
        "- Each row becomes a valid probability distribution, so the weighted sum of values is a proper weighted average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R4gdRqj8W4Y"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 4: Define the softmax function\n",
        "# -------------------------------------------------------------------\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute softmax along the last axis (each row independently).\n",
        "    \n",
        "    For each row:\n",
        "      1. Exponentiate every element: e^(x_i)\n",
        "      2. Sum all exponentiated values in that row: Σ e^(x_j)\n",
        "      3. Divide each exponentiated value by the sum → probabilities\n",
        "    \n",
        "    The .T (transpose) trick:\n",
        "      - np.exp(x) computes e^(x_i) for all elements\n",
        "      - np.sum(np.exp(x), axis=-1) sums across the last axis (each row), giving shape (L,)\n",
        "      - We transpose, divide (broadcasting divides each column by the row sum), \n",
        "        then transpose back to get the correct shape (L, L)\n",
        "    \n",
        "    Result: Each row sums to 1.0 — a valid probability distribution.\n",
        "    For -inf inputs: e^(-inf) = 0, so masked positions get 0 probability.\n",
        "    \"\"\"\n",
        "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5eg2zPy41sP"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 5: Compute the attention weights\n",
        "# -------------------------------------------------------------------\n",
        "# Apply softmax to (scaled scores + mask):\n",
        "#   attention = softmax( (Q·K^T)/√d_k + M )\n",
        "#\n",
        "# Result shape: (4, 4) — the attention weight matrix\n",
        "#   - attention[i][j] = probability that word i attends to word j\n",
        "#   - Each row sums to 1.0\n",
        "#   - Upper triangle is 0 (due to mask → -inf → softmax → 0)\n",
        "#   - This means each word only attends to itself and previous words (causal)\n",
        "attention = softmax(scaled + mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sauNmfl-1TB",
        "outputId": "46b22beb-9034-4c7c-8d56-04209d2581c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.51359112, 0.48640888, 0.        , 0.        ],\n",
              "       [0.53753304, 0.27144826, 0.1910187 , 0.        ],\n",
              "       [0.19293995, 0.03256643, 0.57960627, 0.19488734]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Display the attention weight matrix\n",
        "# -------------------------------------------------------------------\n",
        "# Each row i shows HOW MUCH word i attends to every other word:\n",
        "#   Row 0: [1.0,   0,    0,    0  ] ← word 0 can only see itself (100%)\n",
        "#   Row 1: [0.3, 0.7,   0,    0  ] ← word 1 splits attention between word 0 and itself\n",
        "#   Row 2: [0.1, 0.2, 0.7,   0  ] ← word 2 distributes attention across words 0-2\n",
        "#   Row 3: [0.1, 0.2, 0.3, 0.4]   ← word 3 attends to all words 0-3\n",
        "# (Actual values depend on the random Q, K inputs)\n",
        "#\n",
        "# Notice: upper triangle is always 0 (masked future words!)\n",
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAy37go56LZo",
        "outputId": "78d97fa1-e0b3-4c1d-8294-bf0fdb77f199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.00368231,  1.43739233, -0.59614565, -1.23171219,  1.12030717,\n",
              "        -0.98620738, -0.15461465, -1.03106383],\n",
              "       [ 0.41440401, -0.13671232,  0.02128364, -0.60532081,  0.49977893,\n",
              "        -1.1936286 , -0.27463831, -1.10169151],\n",
              "       [ 0.32673907,  0.72121642, -0.00947672, -0.59897862,  0.90155754,\n",
              "        -0.88535361, -0.21384855, -0.7053796 ],\n",
              "       [ 0.18700384,  1.67754576,  0.33105314, -0.41795742,  1.4258469 ,\n",
              "        -0.18788199, -0.10285145,  0.54683565]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 6: Compute the new context-aware Value vectors\n",
        "# -------------------------------------------------------------------\n",
        "# new_v = attention · V\n",
        "#   attention shape: (L, L) = (4, 4)   — attention weights\n",
        "#   v shape:         (L, d_v) = (4, 8) — original value vectors\n",
        "#   Result:          (L, d_v) = (4, 8) — new value vectors enriched with context\n",
        "#\n",
        "# What this does for each word i:\n",
        "#   new_v[i] = Σ_j  attention[i][j] * v[j]\n",
        "#\n",
        "#   = weighted sum of ALL value vectors, where the weights are the attention scores\n",
        "#\n",
        "# Example for word 2:\n",
        "#   new_v[2] = 0.1*v[0] + 0.2*v[1] + 0.7*v[2] + 0.0*v[3]\n",
        "#   (word 2 gets 70% of its own info, 20% from word 1, 10% from word 0, 0% from word 3)\n",
        "#\n",
        "# This is the KEY output of self-attention: each word's new representation\n",
        "# now incorporates context from the words it attended to!\n",
        "new_v = np.matmul(attention, v)\n",
        "new_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCP2aZOU9VrT",
        "outputId": "e1fe2137-cd95-4a4b-fa1a-3ec21c38104c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.00368231,  1.43739233, -0.59614565, -1.23171219,  1.12030717,\n",
              "        -0.98620738, -0.15461465, -1.03106383],\n",
              "       [ 0.85585446, -1.79878344,  0.67321704,  0.05607552, -0.15542661,\n",
              "        -1.41264124, -0.40136933, -1.17626611],\n",
              "       [ 0.50465335,  2.28693419,  0.67128338,  0.2506863 ,  1.78802234,\n",
              "         0.14775751, -0.11405725,  0.88026286],\n",
              "       [-0.68069105,  0.68385101,  0.17994557, -1.68013201,  0.91543969,\n",
              "        -0.19108312,  0.03160471,  1.40527326]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Compare: Original V vs New V (context-enriched)\n",
        "# -------------------------------------------------------------------\n",
        "# The ORIGINAL value vectors — each word's representation in isolation,\n",
        "# with NO context from other words.\n",
        "# Compare this with new_v above to see how self-attention has mixed\n",
        "# information from different words based on the attention weights.\n",
        "#\n",
        "# Key insight: new_v[i] ≠ v[i] because attention has blended in\n",
        "# information from other words that are relevant to word i.\n",
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_JndWelLDNW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSiJuBQELFHT"
      },
      "source": [
        "# Complete Self-Attention Function\n",
        "\n",
        "Below, we encapsulate the entire self-attention pipeline into a reusable function.  \n",
        "This function works for **both encoder and decoder**:\n",
        "- **Encoder:** Call without mask → every word attends to every other word (full bidirectional context).\n",
        "- **Decoder:** Call with mask → each word can only attend to itself and previous words (causal/autoregressive).\n",
        "\n",
        "In a real Transformer with **multi-headed attention**, this function would be called multiple times (once per head) with different learned projections of Q, K, V, and the outputs would be concatenated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvTnmdcB_jdq"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# COMPLETE SCALED DOT-PRODUCT ATTENTION FUNCTION\n",
        "# ===================================================================\n",
        "# This encapsulates the entire self-attention pipeline into one function.\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Row-wise softmax: converts each row of scores into a probability distribution.\n",
        "    \n",
        "    Args:\n",
        "        x: Input matrix of shape (L, L) — raw attention scores\n",
        "    Returns:\n",
        "        Matrix of same shape where each row sums to 1.0\n",
        "    \"\"\"\n",
        "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Compute Scaled Dot-Product Attention.\n",
        "    \n",
        "    This is the CORE attention mechanism used in every Transformer layer.\n",
        "    \n",
        "    Formula: Attention(Q, K, V) = softmax( (Q · K^T) / √d_k + mask ) · V\n",
        "    \n",
        "    Args:\n",
        "        q:    Query matrix  — shape (L, d_k) — \"what am I looking for?\"\n",
        "        k:    Key matrix    — shape (L, d_k) — \"what do I contain?\"\n",
        "        v:    Value matrix  — shape (L, d_v) — \"what information do I provide?\"\n",
        "        mask: Optional mask — shape (L, L)   — additive mask (0 for allowed, -inf for blocked)\n",
        "              • Pass None for ENCODER (no masking, full bidirectional attention)\n",
        "              • Pass lower-triangular mask for DECODER (causal, can't see future)\n",
        "    \n",
        "    Returns:\n",
        "        out:       New context-enriched value vectors — shape (L, d_v)\n",
        "        attention: Attention weight matrix — shape (L, L), each row sums to 1\n",
        "    \n",
        "    Steps:\n",
        "        1. Compute Q · K^T          → raw affinity scores        (L, L)\n",
        "        2. Divide by √d_k           → stabilize variance         (L, L)\n",
        "        3. Add mask (if provided)    → block future positions     (L, L)\n",
        "        4. Apply softmax             → convert to probabilities   (L, L)\n",
        "        5. Multiply by V             → weighted sum of values     (L, d_v)\n",
        "    \"\"\"\n",
        "    # Step 1 & 2: Scaled dot product\n",
        "    d_k = q.shape[-1]                                  # Get the dimension from Q's last axis\n",
        "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)       # (L, L) — scaled attention scores\n",
        "    \n",
        "    # Step 3: Apply mask (only in decoder, skip in encoder)\n",
        "    if mask is not None:\n",
        "        scaled = scaled + mask   # -inf entries → will become 0 after softmax\n",
        "    \n",
        "    # Step 4: Softmax — convert scores to probabilities\n",
        "    attention = softmax(scaled)  # (L, L) — each row is a probability distribution\n",
        "    \n",
        "    # Step 5: Weighted sum of value vectors\n",
        "    out = np.matmul(attention, v)  # (L, d_v) — new context-aware representations\n",
        "    \n",
        "    return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSxLkZdiSLMT",
        "outputId": "ca70508d-fb6e-4eec-acb6-7a89a60dffa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[ 0.11672673 -2.54870451 -1.44065948  0.93661829  1.36278968  1.04252277\n",
            "  -0.01310938 -1.3163937 ]\n",
            " [ 0.26721599 -0.90218255  0.07417847 -0.10430246  0.52684253 -0.07081531\n",
            "  -0.60511725 -0.55225527]\n",
            " [-0.93297509  0.28724456  1.37184579  0.41589874  0.34981245 -0.24753755\n",
            "  -1.24497125  0.05044148]\n",
            " [-0.11414585 -0.01545749 -0.58376828 -0.40193907  0.93931836 -1.94334363\n",
            "  -0.34770465  1.50103406]]\n",
            "K\n",
            " [[ 1.1226585  -0.85645535  0.54315044  1.36560451  0.52539476 -0.94502504\n",
            "  -0.48444661  0.46268014]\n",
            " [-0.53713766 -1.16937329 -0.57988617  0.92713577 -0.85995607 -0.40352635\n",
            "   0.26555146 -1.83159914]\n",
            " [-2.06994435 -0.09514715 -1.64928361 -0.17375184  0.13146819 -1.76335363\n",
            "   1.56568846  0.69751826]\n",
            " [ 0.32910684 -0.1939204  -0.80444134  0.78816869  0.35599408  0.28309835\n",
            "  -0.25970963  1.49744622]]\n",
            "V\n",
            " [[-0.00368231  1.43739233 -0.59614565 -1.23171219  1.12030717 -0.98620738\n",
            "  -0.15461465 -1.03106383]\n",
            " [ 0.85585446 -1.79878344  0.67321704  0.05607552 -0.15542661 -1.41264124\n",
            "  -0.40136933 -1.17626611]\n",
            " [ 0.50465335  2.28693419  0.67128338  0.2506863   1.78802234  0.14775751\n",
            "  -0.11405725  0.88026286]\n",
            " [-0.68069105  0.68385101  0.17994557 -1.68013201  0.91543969 -0.19108312\n",
            "   0.03160471  1.40527326]]\n",
            "New V\n",
            " [[-0.00368231  1.43739233 -0.59614565 -1.23171219  1.12030717 -0.98620738\n",
            "  -0.15461465 -1.03106383]\n",
            " [ 0.41440401 -0.13671232  0.02128364 -0.60532081  0.49977893 -1.1936286\n",
            "  -0.27463831 -1.10169151]\n",
            " [ 0.32673907  0.72121642 -0.00947672 -0.59897862  0.90155754 -0.88535361\n",
            "  -0.21384855 -0.7053796 ]\n",
            " [ 0.18700384  1.67754576  0.33105314 -0.41795742  1.4258469  -0.18788199\n",
            "  -0.10285145  0.54683565]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.51359112 0.48640888 0.         0.        ]\n",
            " [0.53753304 0.27144826 0.1910187  0.        ]\n",
            " [0.19293995 0.03256643 0.57960627 0.19488734]]\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Run the complete self-attention function and display all results\n",
        "# -------------------------------------------------------------------\n",
        "# Call with mask=mask → Decoder-style causal attention\n",
        "# (To use for Encoder, simply pass mask=None for full bidirectional attention)\n",
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "\n",
        "# Display all inputs and outputs for comparison:\n",
        "print(\"Q\\n\", q)            # Original query vectors (what each word is looking for)\n",
        "print(\"K\\n\", k)            # Original key vectors (what each word offers)\n",
        "print(\"V\\n\", v)            # Original value vectors (each word's raw information)\n",
        "print(\"New V\\n\", values)   # Context-enriched value vectors (after attention blending)\n",
        "print(\"Attention\\n\", attention)  # Attention weights — shows how much each word attends to others\n",
        "                                 # Row i, Col j = how much word i attends to word j\n",
        "                                 # Upper triangle should be 0 (masked future words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HtQQtB2LJus"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
