{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMNu7Jg5troY"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "This notebook will code positional encoding for Transformer neural networks with pytrch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3iqZxn20a7m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Maximum number of tokens (words) in a single input sequence.\n",
        "# In a real Transformer this is typically 512 or more; we use 10 here for easy visualization.\n",
        "max_sequence_length = 10\n",
        "\n",
        "# The size of each token's embedding vector.\n",
        "# Every word is represented as a dense vector of this many dimensions.\n",
        "# In the original \"Attention Is All You Need\" paper this is 512;\n",
        "# we use 6 here so the tensors are small enough to inspect by hand.\n",
        "d_model = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aUNCBlKvxew"
      },
      "source": [
        "$$\n",
        "PE(\\text{position}, 2i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE(\\text{position}, 2i+1) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
        "$$\n",
        "\n",
        "We can rewrite these as\n",
        "\n",
        "$$\n",
        "PE(\\text{position}, i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{i}{d_{model}}} \\bigg) \\text{ when i is even}\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE(\\text{position}, i) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{i-1}{d_{model}}} \\bigg) \\text{ when i is odd}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3SWE1Nxwo-D",
        "outputId": "5d9482c7-79ad-46cc-ce14-8e7ad7e335b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 2., 4.])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate even dimension indices: 0, 2, 4, ... up to (but not including) d_model.\n",
        "# These correspond to the \"2i\" term in the positional encoding formula.\n",
        "# For d_model=6 this produces tensor([0, 2, 4]).\n",
        "# .float() converts to float so we can use them in division later.\n",
        "even_i = torch.arange(0, d_model, 2).float()\n",
        "even_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-RWR30KxdLM",
        "outputId": "123b4f15-8cba-4eb7-9f5d-faabe2dbc286"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compute the denominator for the EVEN-index positional encoding formula:\n",
        "#   denominator = 10000 ^ (2i / d_model)\n",
        "#\n",
        "# For each even dimension index i, the denominator grows exponentially.\n",
        "# Low-index dimensions get small denominators → high-frequency sine waves,\n",
        "# while high-index dimensions get large denominators → low-frequency waves.\n",
        "# This spread of frequencies lets the model distinguish positions at\n",
        "# many different scales (nearby words vs. far-apart words).\n",
        "even_denominator = torch.pow(10000, even_i / d_model)\n",
        "even_denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iITvtjTt6jO-",
        "outputId": "2a9f433f-14b6-4a42-fcb2-4b896e215de0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 3., 5.])"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate odd dimension indices: 1, 3, 5, ... up to d_model.\n",
        "# These correspond to the \"2i+1\" term in the positional encoding formula,\n",
        "# where cosine is applied instead of sine.\n",
        "# For d_model=6 this produces tensor([1, 3, 5]).\n",
        "odd_i = torch.arange(1, d_model, 2).float()\n",
        "odd_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAIVnPLJ1JYC",
        "outputId": "67ca824f-82b1-41d8-fac5-84ff84999349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compute the denominator for the ODD-index formula:\n",
        "#   denominator = 10000 ^ ((i-1) / d_model)\n",
        "#\n",
        "# Notice that (odd_i - 1) gives [0, 2, 4] — exactly the same as even_i!\n",
        "# So the denominators for even and odd positions turn out to be identical.\n",
        "# This is because dimension pair (2i, 2i+1) shares the same frequency;\n",
        "# only the function changes (sin vs cos).\n",
        "even_denominator = torch.pow(10000, (odd_i - 1) / d_model)\n",
        "even_denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBtyLN2NAtEC"
      },
      "source": [
        "`even_denominator` and `odd_denominator` are the same! So we can just do one of these actions and call the resulting variable `denominator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyjRI1imBA3F"
      },
      "outputs": [],
      "source": [
        "# Since both even and odd denominators are identical,\n",
        "# we only need one variable. We'll call it 'denominator'\n",
        "# and reuse it for both the sin (even) and cos (odd) calculations.\n",
        "denominator = even_denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwxxoNSN-me9"
      },
      "outputs": [],
      "source": [
        "# Create a column vector of position indices: [[0], [1], [2], ..., [max_sequence_length-1]]\n",
        "# Shape: (max_sequence_length, 1)  →  (10, 1)\n",
        "#\n",
        "# Each row represents one token's position in the sequence.\n",
        "# The reshape to a column vector is essential so that when we later divide\n",
        "# by the denominator (shape (3,)), PyTorch broadcasts the operation to\n",
        "# produce a (10, 3) matrix — one value per (position, frequency) pair.\n",
        "position = torch.arange(max_sequence_length, dtype=torch.float).reshape(max_sequence_length, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nfvUzG8-rMK",
        "outputId": "3bc44cf5-0c38-43ae-bee2-7cd86f078601"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [6.],\n",
              "        [7.],\n",
              "        [8.],\n",
              "        [9.]])"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the position column vector.\n",
        "# You should see values 0 through 9, each in its own row:\n",
        "# tensor([[0.], [1.], [2.], ..., [9.]])\n",
        "position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEm-9niG4VEl"
      },
      "outputs": [],
      "source": [
        "# ---- Core positional encoding calculation ----\n",
        "#\n",
        "# EVEN dimensions (0, 2, 4):  PE(pos, 2i)   = sin(pos / denominator)\n",
        "# ODD  dimensions (1, 3, 5):  PE(pos, 2i+1) = cos(pos / denominator)\n",
        "#\n",
        "# Broadcasting:\n",
        "#   position   shape: (10, 1)\n",
        "#   denominator shape: (3,)   →  broadcast to (10, 3)\n",
        "#\n",
        "# Result: each is a (10, 3) matrix — 10 positions × 3 frequency channels.\n",
        "# even_PE holds the sin values for dimensions 0, 2, 4\n",
        "# odd_PE  holds the cos values for dimensions 1, 3, 5\n",
        "even_PE = torch.sin(position / denominator)\n",
        "odd_PE = torch.cos(position / denominator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqfc01YJ43w6",
        "outputId": "b151d8fe-ce1c-4bfc-bb57-1e3ec54aae49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000],\n",
              "        [ 0.8415,  0.0464,  0.0022],\n",
              "        [ 0.9093,  0.0927,  0.0043],\n",
              "        [ 0.1411,  0.1388,  0.0065],\n",
              "        [-0.7568,  0.1846,  0.0086],\n",
              "        [-0.9589,  0.2300,  0.0108],\n",
              "        [-0.2794,  0.2749,  0.0129],\n",
              "        [ 0.6570,  0.3192,  0.0151],\n",
              "        [ 0.9894,  0.3629,  0.0172],\n",
              "        [ 0.4121,  0.4057,  0.0194]])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the sine-based (even) encodings.\n",
        "# Shape is (10, 3) — 10 positions, 3 even dimensions.\n",
        "# Notice column 0 changes fast (high freq) while column 2 changes slowly (low freq).\n",
        "even_PE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjmx56D25A5T",
        "outputId": "9615f471-3bc5-445e-d229-09bcd93d239e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Confirm shape: (max_sequence_length, d_model/2) = (10, 3)\n",
        "even_PE.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8TlRfY745hA",
        "outputId": "bfd9bd54-009a-4cb5-c682-88bffc6b7a4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.0000,  1.0000,  1.0000],\n",
              "        [ 0.5403,  0.9989,  1.0000],\n",
              "        [-0.4161,  0.9957,  1.0000],\n",
              "        [-0.9900,  0.9903,  1.0000],\n",
              "        [-0.6536,  0.9828,  1.0000],\n",
              "        [ 0.2837,  0.9732,  0.9999],\n",
              "        [ 0.9602,  0.9615,  0.9999],\n",
              "        [ 0.7539,  0.9477,  0.9999],\n",
              "        [-0.1455,  0.9318,  0.9999],\n",
              "        [-0.9111,  0.9140,  0.9998]])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the cosine-based (odd) encodings.\n",
        "# Same shape (10, 3). These will be interleaved with even_PE next.\n",
        "odd_PE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bazd5CSZ948R",
        "outputId": "396408c7-26b1-4268-b530-bcfa32a95d69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Confirm shape: (10, 3) — matches even_PE\n",
        "odd_PE.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0n6V1nk_Pgg",
        "outputId": "0b9c0f1e-3a48-4993-a295-5634b73dd82c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 2])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Stack even_PE and odd_PE along a NEW third dimension (dim=2).\n",
        "#\n",
        "# Before stacking:\n",
        "#   even_PE shape: (10, 3)   — sin values for dims 0, 2, 4\n",
        "#   odd_PE  shape: (10, 3)   — cos values for dims 1, 3, 5\n",
        "#\n",
        "# After stacking:\n",
        "#   stacked shape: (10, 3, 2)\n",
        "#   For each position and each frequency channel, we now have a pair:\n",
        "#   [sin_value, cos_value]\n",
        "#\n",
        "# This arrangement prepares us to interleave them: [sin0, cos0, sin1, cos1, sin2, cos2]\n",
        "stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "stacked.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJAGzwSF_fVV",
        "outputId": "62e15b26-d4b0-433d-8fd0-fc10b5e7311b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Flatten dimensions 1 and 2 to interleave the sin/cos pairs.\n",
        "#\n",
        "# stacked shape: (10, 3, 2)  →  flattened to (10, 6)\n",
        "#\n",
        "# The flatten merges the last two dims, so each row becomes:\n",
        "#   [sin_dim0, cos_dim1, sin_dim2, cos_dim3, sin_dim4, cos_dim5]\n",
        "#\n",
        "# This is exactly the positional encoding matrix PE of shape\n",
        "# (max_sequence_length, d_model) = (10, 6).\n",
        "#\n",
        "# In a real Transformer, this matrix is ADDED element-wise to the\n",
        "# word embedding matrix so that each token's vector carries both\n",
        "# semantic meaning (from the embedding) and position information.\n",
        "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "PE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Liidl3ggt0wK"
      },
      "source": [
        "## Class\n",
        "\n",
        "Let's combine all the code above into a cute class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1G1ziOa6SdZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Generates the sinusoidal positional encoding matrix described in\n",
        "    \"Attention Is All You Need\" (Vaswani et al., 2017).\n",
        "\n",
        "    The encoding injects position information into token embeddings so the\n",
        "    Transformer can distinguish token order (since it has no recurrence or\n",
        "    convolution to capture sequence ordering on its own).\n",
        "\n",
        "    Output shape: (max_sequence_length, d_model)\n",
        "    Each row is the positional encoding vector for that position index.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model (int): Dimensionality of the token embeddings.\n",
        "                           Must be even so that dimensions split evenly\n",
        "                           into sin (even indices) and cos (odd indices).\n",
        "            max_sequence_length (int): The longest sequence the model will\n",
        "                           handle. Determines the number of rows in the\n",
        "                           output matrix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        Compute and return the positional encoding matrix.\n",
        "\n",
        "        Steps:\n",
        "        1. Build even dimension indices [0, 2, 4, ...] to compute\n",
        "           the shared denominator  10000^(2i / d_model).\n",
        "        2. Create a column vector of positions [0, 1, ..., max_seq_len-1].\n",
        "        3. Apply sin to even dimensions and cos to odd dimensions.\n",
        "        4. Interleave the results so that the final tensor has the layout:\n",
        "           [sin_dim0, cos_dim1, sin_dim2, cos_dim3, ...]\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (max_sequence_length, d_model) containing the\n",
        "            positional encodings. In practice this tensor is added\n",
        "            element-wise to the word embedding matrix before it enters\n",
        "            the self-attention layers.\n",
        "        \"\"\"\n",
        "        # Step 1: Even dimension indices → [0, 2, 4, ..., d_model-2]\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "\n",
        "        # Denominator = 10000^(2i / d_model).\n",
        "        # Low-index dims get small denominators → high-frequency waves.\n",
        "        # High-index dims get large denominators → low-frequency waves.\n",
        "        # This spread of frequencies allows the model to capture both\n",
        "        # fine-grained (nearby tokens) and coarse (distant tokens) position info.\n",
        "        denominator = torch.pow(10000, even_i / self.d_model)\n",
        "\n",
        "        # Step 2: Position column vector — shape (max_seq_len, 1)\n",
        "        # Reshaped so that division by denominator broadcasts to (max_seq_len, d_model/2).\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "\n",
        "        # Step 3: Apply sin and cos\n",
        "        even_PE = torch.sin(position / denominator)   # shape: (max_seq_len, d_model/2)\n",
        "        odd_PE  = torch.cos(position / denominator)    # shape: (max_seq_len, d_model/2)\n",
        "\n",
        "        # Step 4: Interleave sin and cos values\n",
        "        # stack → (max_seq_len, d_model/2, 2)  then flatten last two dims → (max_seq_len, d_model)\n",
        "        # Result layout per row: [sin0, cos0, sin1, cos1, sin2, cos2, ...]\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "        return PE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENHY3b-BDgL9",
        "outputId": "6413d8c5-7fbd-48b4-dfd6-bbae0396bd0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the PositionalEncoding class with the same small values\n",
        "# we used above (d_model=6, max_sequence_length=10) and call forward()\n",
        "# to generate the (10, 6) encoding matrix.\n",
        "#\n",
        "# In a full Transformer pipeline you would do:\n",
        "#   encoded_input = word_embeddings + pe.forward()\n",
        "# so that each token vector now carries both meaning and position.\n",
        "pe = PositionalEncoding(d_model=6, max_sequence_length=10)\n",
        "pe.forward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjPIbLcBt6l4"
      },
      "source": [
        "Happy Coding!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
