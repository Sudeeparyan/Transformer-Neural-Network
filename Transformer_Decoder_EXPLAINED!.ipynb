{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke5x2l_I0Kmh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMER DECODER - COMPLETE IMPLEMENTATION\n",
        "# =============================================================================\n",
        "# The Decoder is the second half of the Transformer architecture (Encoder-Decoder).\n",
        "# Its job is to take context-aware embeddings from the Encoder (source language, e.g. English)\n",
        "# and sequentially predict output tokens in the target language (e.g. Kannada).\n",
        "#\n",
        "# Architecture flow per Decoder Layer:\n",
        "#   1. Masked Multi-Head Self-Attention (prevents peeking at future tokens)\n",
        "#   2. Add & Layer Normalization (residual/skip connection)\n",
        "#   3. Multi-Head Cross-Attention (attends to Encoder output)\n",
        "#   4. Add & Layer Normalization\n",
        "#   5. Position-wise Feed-Forward Network\n",
        "#   6. Add & Layer Normalization\n",
        "#\n",
        "# Key dimensions used throughout (with default hyperparameters):\n",
        "#   batch_size = 30, sequence_length = 200, d_model = 512, num_heads = 8\n",
        "#   head_dim = d_model // num_heads = 64, ffn_hidden = 2048\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Computes Scaled Dot-Product Attention.\n",
        "    \n",
        "    This is the core attention mechanism: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
        "    \n",
        "    Why scale by sqrt(d_k)?\n",
        "      - Without scaling, the dot products grow large in magnitude for high-dimensional vectors,\n",
        "        pushing softmax into regions with extremely small gradients. Scaling keeps values\n",
        "        in a range where softmax gradients are healthy for training.\n",
        "    \n",
        "    Args:\n",
        "        q: Query tensor  - shape: (batch, heads, seq_len, head_dim) e.g. 30 x 8 x 200 x 64\n",
        "        k: Key tensor    - shape: (batch, heads, seq_len, head_dim) e.g. 30 x 8 x 200 x 64\n",
        "        v: Value tensor  - shape: (batch, heads, seq_len, head_dim) e.g. 30 x 8 x 200 x 64\n",
        "        mask: Optional look-ahead mask - shape: (seq_len, seq_len) e.g. 200 x 200\n",
        "              Upper triangular matrix of -inf values to prevent attending to future tokens.\n",
        "    \n",
        "    Returns:\n",
        "        values: Weighted sum of value vectors    - shape: (batch, heads, seq_len, head_dim)\n",
        "        attention: Attention weight matrix        - shape: (batch, heads, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # Get the dimension of keys (d_k = 64) for scaling\n",
        "    d_k = q.size()[-1] \n",
        "    \n",
        "    # Step 1: Compute raw attention scores via dot product of Q and K^T\n",
        "    # Q @ K^T gives a (seq_len x seq_len) matrix showing how much each token attends to every other\n",
        "    # Divide by sqrt(d_k) to prevent vanishing gradients in softmax\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
        "    print(f\"scaled.size() : {scaled.size()}\")\n",
        "    \n",
        "    # Step 2: Apply the look-ahead mask (if provided)\n",
        "    # The mask is an upper triangular matrix filled with -inf.\n",
        "    # Adding -inf to future positions makes their softmax output ~0,\n",
        "    # effectively preventing the decoder from \"cheating\" by looking at future tokens.\n",
        "    if mask is not None:\n",
        "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
        "        scaled += mask # Broadcasting: 200x200 mask applied across batch & heads → 30 x 8 x 200 x 200\n",
        "    \n",
        "    # Step 3: Apply softmax to get attention weights (probabilities that sum to 1 per row)\n",
        "    # dim=-1 means softmax is applied along the last dimension (key positions)\n",
        "    attention = F.softmax(scaled, dim=-1) # 30 x 8 x 200 x 200\n",
        "    \n",
        "    # Step 4: Multiply attention weights by Value vectors to get the weighted output\n",
        "    # Each token's output is a weighted combination of all Value vectors it can attend to\n",
        "    values = torch.matmul(attention, v) # 30 x 8 x 200 x 64\n",
        "    return values, attention\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network (FFN).\n",
        "    \n",
        "    Applied independently to each position (token) in the sequence.\n",
        "    Consists of two linear transformations with a ReLU activation in between:\n",
        "        FFN(x) = Linear2(Dropout(ReLU(Linear1(x))))\n",
        "    \n",
        "    Purpose: Adds non-linearity and allows the model to learn complex transformations\n",
        "    beyond what attention alone can capture. The hidden layer expands the dimension\n",
        "    (512 → 2048) to give the network more capacity, then projects back (2048 → 512).\n",
        "    \n",
        "    This is sometimes called a \"two-layer MLP\" applied at each position.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)   # Expansion: 512 → 2048\n",
        "        self.linear2 = nn.Linear(hidden, d_model)    # Projection: 2048 → 512\n",
        "        self.relu = nn.ReLU()                         # Non-linear activation\n",
        "        self.dropout = nn.Dropout(p=drop_prob)        # Regularization to prevent overfitting\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x: (batch, seq_len, d_model) = 30 x 200 x 512\n",
        "        x = self.linear1(x)  # Expand to higher dimension: 30 x 200 x 2048\n",
        "        print(f\"x after first linear layer: {x.size()}\")\n",
        "        x = self.relu(x)     # Apply ReLU: introduces non-linearity, zeroes out negatives\n",
        "        print(f\"x after relu layer: {x.size()}\")\n",
        "        x = self.dropout(x)  # Randomly zero elements during training for regularization\n",
        "        print(f\"x after dropout layer: {x.size()}\")\n",
        "        x = self.linear2(x)  # Project back to model dimension: 30 x 200 x 512\n",
        "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "        return x  # Output: 30 x 200 x 512 (same shape as input — enables residual connection)\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer Normalization.\n",
        "    \n",
        "    Normalizes activations across the feature (embedding) dimension for each token independently.\n",
        "    For each token vector of 512 dimensions, it computes the mean and std, then normalizes\n",
        "    to zero mean and unit variance.\n",
        "    \n",
        "    Formula: LayerNorm(x) = gamma * ((x - mean) / std) + beta\n",
        "    \n",
        "    Why Layer Norm?\n",
        "      - Reduces \"internal covariate shift\": stabilizes the distribution of activations\n",
        "        across layers, helping the model converge faster and more reliably.\n",
        "      - gamma (scale) and beta (shift) are LEARNABLE parameters, allowing the model\n",
        "        to undo the normalization if that's beneficial for a particular feature.\n",
        "    \n",
        "    Unlike Batch Normalization, Layer Norm normalizes across features (not across the batch),\n",
        "    making it suitable for sequence models where batch statistics are unreliable.\n",
        "    \"\"\"\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape = parameters_shape  # [512] — the embedding dimension to normalize over\n",
        "        self.eps = eps                             # Small constant to avoid division by zero\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))   # Learnable scale, initialized to 1\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))   # Learnable shift, initialized to 0\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: (batch, seq_len, d_model) = 30 x 200 x 512\n",
        "        \n",
        "        # Determine which dimensions to normalize over (last N dims matching parameters_shape)\n",
        "        # For parameters_shape=[512], dims=[-1], meaning normalize across the embedding dimension\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]  # [-1]\n",
        "        print(f\"dims: {dims}\")\n",
        "        \n",
        "        # Compute mean across the embedding dimension for each token\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)  # 30 x 200 x 1\n",
        "        print(f\"Mean ({mean.size()})\")\n",
        "        \n",
        "        # Compute variance, then standard deviation (with epsilon for numerical stability)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)  # 30 x 200 x 1\n",
        "        std = (var + self.eps).sqrt()  # 30 x 200 x 1\n",
        "        print(f\"Standard Deviation  ({std.size()})\")\n",
        "        \n",
        "        # Normalize: zero mean, unit variance per token embedding\n",
        "        y = (inputs - mean) / std  # 30 x 200 x 512\n",
        "        print(f\"y: {y.size()}\")\n",
        "        \n",
        "        # Apply learnable affine transformation: scale by gamma, shift by beta\n",
        "        # This lets the model learn the optimal scale/shift for each feature\n",
        "        out = self.gamma * y + self.beta  # 30 x 200 x 512\n",
        "        print(f\"out: {out.size()}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention.\n",
        "    \n",
        "    Instead of computing a single attention function, the model projects Q, K, V\n",
        "    into 'num_heads' different subspaces (each of dimension head_dim = d_model // num_heads),\n",
        "    performs attention independently in each subspace (\"head\"), then concatenates and\n",
        "    projects the results.\n",
        "    \n",
        "    Why multiple heads?\n",
        "      - Each head can learn to attend to different aspects of the input\n",
        "        (e.g., one head might focus on syntax, another on semantics).\n",
        "      - With 8 heads × 64 dims each = 512 total dims, we get diverse attention patterns\n",
        "        at the same computational cost as a single 512-dim attention.\n",
        "    \n",
        "    In SELF-attention: Q, K, V all come from the SAME input (the decoder's own embeddings).\n",
        "    A single linear layer produces all three (Q, K, V) concatenated for efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model       # 512 — total embedding dimension\n",
        "        self.num_heads = num_heads   # 8 — number of parallel attention heads\n",
        "        self.head_dim = d_model // num_heads  # 64 — dimension per head\n",
        "        \n",
        "        # Single linear layer that produces Q, K, V concatenated (3 × 512 = 1536)\n",
        "        # More efficient than 3 separate linear layers\n",
        "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)  # 512 → 1536\n",
        "        \n",
        "        # Final linear projection after concatenating all heads\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)    # 512 → 512\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (batch, seq_len, d_model) — decoder embeddings\n",
        "            mask: Look-ahead mask (seq_len, seq_len) — prevents attending to future tokens\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, d_model = x.size()  # 30 x 200 x 512 \n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        \n",
        "        # Step 1: Project input to Q, K, V in one shot\n",
        "        qkv = self.qkv_layer(x)  # 30 x 200 x 1536\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        \n",
        "        # Step 2: Reshape to separate the heads\n",
        "        # Split the last dim (1536) into num_heads (8) × 3*head_dim (192)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)  # 30 x 200 x 8 x 192\n",
        "        print(f\"qkv after reshape .size(): {qkv.size()}\")\n",
        "        \n",
        "        # Step 3: Permute so heads become the second dimension\n",
        "        # This allows parallel attention computation across all heads\n",
        "        qkv = qkv.permute(0, 2, 1, 3)  # 30 x 8 x 200 x 192\n",
        "        print(f\"qkv after permutation: {qkv.size()}\")\n",
        "        \n",
        "        # Step 4: Split the last dimension into Q, K, V (each 64 dims)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)  # Each: 30 x 8 x 200 x 64\n",
        "        print(f\"q: {q.size()}, k:{k.size()}, v:{v.size()}\")\n",
        "        \n",
        "        # Step 5: Compute scaled dot-product attention for all heads in parallel\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)  # values: 30 x 8 x 200 x 64\n",
        "        print(f\"values: {values.size()}, attention:{attention.size()}\")\n",
        "        \n",
        "        # Step 6: Concatenate all heads back together\n",
        "        # Reshape from (30, 8, 200, 64) → (30, 200, 512)\n",
        "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)  # 30 x 200 x 512\n",
        "        print(f\"values after reshaping: {values.size()}\")\n",
        "        \n",
        "        # Step 7: Final linear projection to mix information across heads\n",
        "        out = self.linear_layer(values)  # 30 x 200 x 512\n",
        "        print(f\"out after passing through linear layer: {out.size()}\")\n",
        "        return out  # 30 x 200 x 512\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Cross-Attention.\n",
        "    \n",
        "    KEY DIFFERENCE from Self-Attention:\n",
        "      - In SELF-attention: Q, K, V all come from the same source (decoder input).\n",
        "      - In CROSS-attention: Q comes from the DECODER, but K and V come from the ENCODER output.\n",
        "    \n",
        "    This is how the decoder \"looks at\" the source sentence (encoder output) to inform\n",
        "    its predictions. The decoder asks \"what parts of the English sentence should I\n",
        "    attend to in order to predict the next Kannada token?\"\n",
        "    \n",
        "    - x: Encoder output (source language context vectors)\n",
        "    - y: Decoder's current state (target language being generated)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads  # 64\n",
        "        \n",
        "        # K and V come from encoder output → single projection for both (2 × 512 = 1024)\n",
        "        self.kv_layer = nn.Linear(d_model, 2 * d_model)  # 512 → 1024\n",
        "        \n",
        "        # Q comes from decoder state → separate projection\n",
        "        self.q_layer = nn.Linear(d_model, d_model)        # 512 → 512\n",
        "        \n",
        "        # Final projection after concatenating heads\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)    # 512 → 512\n",
        "    \n",
        "    def forward(self, x, y, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Encoder output (batch, seq_len, d_model) — source of Keys and Values\n",
        "            y: Decoder state  (batch, seq_len, d_model) — source of Queries\n",
        "            mask: Optional mask (not typically used in cross-attention during standard training)\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, d_model = x.size()  # 30 x 200 x 512\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        \n",
        "        # Step 1: Generate K, V from encoder output (source sentence)\n",
        "        kv = self.kv_layer(x)  # 30 x 200 x 1024\n",
        "        print(f\"kv.size(): {kv.size()}\")\n",
        "        \n",
        "        # Step 2: Generate Q from decoder state (target sentence being built)\n",
        "        q = self.q_layer(y)  # 30 x 200 x 512\n",
        "        print(f\"q.size(): {q.size()}\")\n",
        "        \n",
        "        # Step 3: Reshape for multi-head processing\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)  # 30 x 200 x 8 x 128\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)         # 30 x 200 x 8 x 64\n",
        "        \n",
        "        # Step 4: Move heads to second dimension for parallel computation\n",
        "        kv = kv.permute(0, 2, 1, 3)  # 30 x 8 x 200 x 128\n",
        "        q = q.permute(0, 2, 1, 3)    # 30 x 8 x 200 x 64\n",
        "        \n",
        "        # Step 5: Split kv into separate K and V tensors\n",
        "        k, v = kv.chunk(2, dim=-1)  # K: 30 x 8 x 200 x 64, V: 30 x 8 x 200 x 64\n",
        "        \n",
        "        # Step 6: Compute attention — decoder queries attend to encoder keys/values\n",
        "        # No mask needed here: decoder should see ALL encoder positions\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)  # 30 x 8 x 200 x 64\n",
        "        print(f\"values: {values.size()}, attention:{attention.size()}\")\n",
        "        \n",
        "        # Step 7: Concatenate heads and project\n",
        "        values = values.reshape(batch_size, sequence_length, d_model)  # 30 x 200 x 512\n",
        "        out = self.linear_layer(values)  # 30 x 200 x 512\n",
        "        print(f\"out after passing through linear layer: {out.size()}\")\n",
        "        return out  # 30 x 200 x 512\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Decoder Layer (one \"block\" in the decoder stack).\n",
        "    \n",
        "    Each decoder layer has 3 sub-layers, each followed by a residual connection + layer norm:\n",
        "    \n",
        "    Sub-layer 1: MASKED Self-Attention\n",
        "        - The decoder attends to its own previous outputs\n",
        "        - Look-ahead mask prevents attending to future positions\n",
        "        - This ensures autoregressive generation (predict one token at a time)\n",
        "    \n",
        "    Sub-layer 2: Cross-Attention (Encoder-Decoder Attention)\n",
        "        - Queries from decoder, Keys/Values from encoder output\n",
        "        - This is how the decoder \"reads\" the source sentence\n",
        "        - No mask needed — decoder can attend to all encoder positions\n",
        "    \n",
        "    Sub-layer 3: Position-wise Feed-Forward Network\n",
        "        - Two linear layers with ReLU, applied independently per position\n",
        "        - Adds non-linear transformation capacity\n",
        "    \n",
        "    RESIDUAL CONNECTIONS (Skip Connections):\n",
        "        - After each sub-layer: output = LayerNorm(sub_layer(x) + x)\n",
        "        - The \"+ x\" part is the residual/skip connection\n",
        "        - Purpose: prevents vanishing gradients in deep networks by providing\n",
        "          a direct path for gradients to flow backward through the network\n",
        "        - Without these, training a 5+ layer decoder would be extremely difficult\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Sub-layer 1: Masked Multi-Head Self-Attention\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Sub-layer 2: Multi-Head Cross-Attention (encoder-decoder attention)\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Sub-layer 3: Position-wise Feed-Forward Network\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, decoder_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Encoder output        (batch, seq_len, d_model) — 30 x 200 x 512\n",
        "            y: Decoder input          (batch, seq_len, d_model) — 30 x 200 x 512\n",
        "            decoder_mask: Look-ahead mask (seq_len, seq_len)    — 200 x 200\n",
        "        Returns:\n",
        "            y: Processed decoder output (batch, seq_len, d_model) — 30 x 200 x 512\n",
        "        \"\"\"\n",
        "        # ---- Sub-layer 1: Masked Self-Attention + Residual + LayerNorm ----\n",
        "        _y = y  # Save input for residual connection (skip connection)\n",
        "        print(\"MASKED SELF ATTENTION\")\n",
        "        y = self.self_attention(y, mask=decoder_mask)  # Self-attend with causal mask\n",
        "        print(\"DROP OUT 1\")\n",
        "        y = self.dropout1(y)  # Regularization\n",
        "        print(\"ADD + LAYER NORMALIZATION 1\")\n",
        "        y = self.norm1(y + _y)  # Residual connection: add original input, then normalize\n",
        "        # The residual connection (y + _y) allows gradients to bypass the attention layer\n",
        "\n",
        "        # ---- Sub-layer 2: Cross-Attention + Residual + LayerNorm ----\n",
        "        _y = y  # Save for next residual connection\n",
        "        print(\"CROSS ATTENTION\")\n",
        "        y = self.encoder_decoder_attention(x, y, mask=None)  # Q from decoder, K/V from encoder\n",
        "        # mask=None because decoder should freely attend to ALL encoder positions\n",
        "        print(\"DROP OUT 2\")\n",
        "        y = self.dropout2(y)\n",
        "        print(\"ADD + LAYER NORMALIZATION 2\")\n",
        "        y = self.norm2(y + _y)  # Residual + normalize\n",
        "\n",
        "        # ---- Sub-layer 3: Feed-Forward + Residual + LayerNorm ----\n",
        "        _y = y  # Save for residual connection\n",
        "        print(\"FEED FORWARD 1\")\n",
        "        y = self.ffn(y)  # Position-wise FFN: expand to 2048, ReLU, project back to 512\n",
        "        print(\"DROP OUT 3\")\n",
        "        y = self.dropout3(y)\n",
        "        print(\"ADD + LAYER NORMALIZATION 3\")\n",
        "        y = self.norm3(y + _y)  # Residual + normalize\n",
        "        \n",
        "        return y  # 30 x 200 x 512 — same shape throughout, enabling layer stacking\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Custom Sequential container for decoder layers.\n",
        "    \n",
        "    PyTorch's nn.Sequential normally passes only a single tensor between layers.\n",
        "    The decoder needs to pass THREE inputs (encoder output, decoder state, mask)\n",
        "    through each layer. This subclass overrides forward() to handle that.\n",
        "    \n",
        "    Note: Only 'y' (decoder state) is updated between layers.\n",
        "    'x' (encoder output) and 'mask' remain unchanged — every decoder layer\n",
        "    receives the same encoder output and the same mask.\n",
        "    \"\"\"\n",
        "    def forward(self, *inputs):\n",
        "        x, y, mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, mask)  # Each layer refines y; x and mask stay the same\n",
        "        return y\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer Decoder.\n",
        "    \n",
        "    Stacks multiple DecoderLayers (default: num_layers=1, typically 6 in the original paper).\n",
        "    Each layer progressively refines the decoder's representation of the target sequence\n",
        "    by repeatedly:\n",
        "      1. Attending to its own previous outputs (self-attention with causal mask)\n",
        "      2. Attending to the encoder's output (cross-attention)\n",
        "      3. Applying a feed-forward transformation\n",
        "    \n",
        "    The output maintains shape (batch, seq_len, d_model) throughout all layers,\n",
        "    which is eventually mapped to vocabulary logits for next-token prediction.\n",
        "    \n",
        "    In the original \"Attention Is All You Need\" paper, num_layers = 6.\n",
        "    Here it's configurable (set to 5 in the demo).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n",
        "        super().__init__()\n",
        "        # Create a stack of num_layers identical (but independently parameterized) decoder layers\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n",
        "                                          for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        # x:    Encoder output  — 30 x 200 x 512  (source language embeddings)\n",
        "        # y:    Decoder input   — 30 x 200 x 512  (target language embeddings)\n",
        "        # mask: Look-ahead mask — 200 x 200        (causal mask to prevent future peeking)\n",
        "        y = self.layers(x, y, mask)\n",
        "        return y  # 30 x 200 x 512 — ready for final linear + softmax to predict tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fvLLVKFzP1t",
        "outputId": "c82a050e-17e6-4925-f3d3-c0ca8d7b2e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# HYPERPARAMETERS & DECODER DEMO\n",
        "# =============================================================================\n",
        "\n",
        "d_model = 512               # Embedding dimension: each token is represented as a 512-dim vector\n",
        "                             # Higher = richer representation but more compute. 512 is from the original paper.\n",
        "\n",
        "num_heads = 8                # Number of parallel attention heads.\n",
        "                             # 512 / 8 = 64 dims per head. Each head learns different attention patterns.\n",
        "\n",
        "drop_prob = 0.1              # Dropout probability: 10% of neurons randomly zeroed during training\n",
        "                             # to prevent overfitting and improve generalization.\n",
        "\n",
        "batch_size = 30              # Number of sentence pairs processed simultaneously.\n",
        "                             # Larger batches = more stable gradients but more memory.\n",
        "\n",
        "max_sequence_length = 200    # Maximum number of tokens per sentence.\n",
        "                             # Sentences shorter than 200 are padded; longer ones are truncated.\n",
        "\n",
        "ffn_hidden = 2048            # Hidden dimension of the feed-forward network (4x d_model).\n",
        "                             # The expansion ratio (512→2048→512) gives the FFN more capacity.\n",
        "\n",
        "num_layers = 5               # Number of stacked decoder layers.\n",
        "                             # More layers = deeper network that can capture more complex patterns.\n",
        "                             # Original paper uses 6; here we use 5 for demonstration.\n",
        "\n",
        "# --- Simulated Inputs ---\n",
        "# In a real model, these would come from:\n",
        "#   x: Encoder output (positional-encoded English sentence embeddings passed through encoder)\n",
        "#   y: Target language embeddings (positional-encoded Kannada sentence embeddings)\n",
        "x = torch.randn( (batch_size, max_sequence_length, d_model) )  # Simulated encoder output (English)\n",
        "y = torch.randn( (batch_size, max_sequence_length, d_model) )  # Simulated decoder input (Kannada)\n",
        "\n",
        "# --- Look-Ahead (Causal) Mask ---\n",
        "# Creates an upper triangular matrix of -inf values:\n",
        "#   [[  0, -inf, -inf, ..., -inf],\n",
        "#    [  0,    0, -inf, ..., -inf],\n",
        "#    [  0,    0,    0, ..., -inf],\n",
        "#    ...\n",
        "#    [  0,    0,    0, ...,    0]]\n",
        "#\n",
        "# Purpose: During training, the decoder sees the ENTIRE target sentence at once.\n",
        "# But at inference time, it generates one token at a time. The mask simulates this\n",
        "# autoregressive behavior during training by preventing position i from attending\n",
        "# to any position j > i (future tokens). Without this mask, the model would \"cheat\"\n",
        "# by looking at the answer while trying to predict it.\n",
        "mask = torch.full([max_sequence_length, max_sequence_length], float('-inf'))\n",
        "mask = torch.triu(mask, diagonal=1)  # Keep upper triangle as -inf, lower triangle + diagonal as 0\n",
        "\n",
        "# --- Build and Run the Decoder ---\n",
        "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "out = decoder(x, y, mask)\n",
        "# out shape: 30 x 200 x 512\n",
        "# This output would then be passed through a final Linear layer (512 → vocab_size)\n",
        "# followed by softmax to get probability distributions over the target vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6MA9JrD2QW9",
        "outputId": "1d529334-e9a2-4ba7-b18b-52148eb44cb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the look-ahead mask matrix.\n",
        "# You'll see 0s on and below the diagonal (positions the token CAN attend to)\n",
        "# and -inf above the diagonal (future positions that are BLOCKED).\n",
        "# Row i represents what token i can see: it can attend to tokens 0..i but not i+1..N.\n",
        "mask"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
